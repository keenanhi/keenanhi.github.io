<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml"
      xmlns:fb="http://ogp.me/ns/fb#">
  <head>
    <meta charset="utf-8">
    <title>Get Mastered: Audio Experts</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Collaborative, human mastering from experienced audio experts. Try our pro-bono mastering, free of charge.">
    <meta name="author" content="Keenan Hye">
    <meta property="og:image" content="getmastered.com/img/social-media/facebook-link-img.jpg" />

    <!-- Bootstrap Core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css">

    <!-- Need for mobile nav collapse  -->
    <link href="/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
      <!-- <link href="/css/animate.css" rel="stylesheet" /> -->
    <!-- CSS -->
    <link href="/css/style.css" rel="stylesheet">
      <!-- <link href="/color/default.css" rel="stylesheet">    -->

  </head>

<body id="page-top" data-spy="scroll" data-target=".navbar-custom" class="bg-white">
    <!-- Preloader -->
    <div id="preloader">
      <div id="load"></div>
    </div>


    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation" style="background: #000; padding: 1px 0;">
        <div class="container-fluid">
            <div class="navbar-left page-scroll col-lg-3" style="padding: 0px;">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="hidden-xs hidden-sm hidden-md brand" href="/" align="right">
                    <h4 style="padding: 15px 20px 5px 0px; margin: 0px; text-transform: uppercase;">Get Mastered</h4>
                </a>

                <a class="hidden-lg hidden-xl brand" href="/" align="left">
                    <h4 style="padding: 15px 20px 5px 0px; margin: 0px; text-transform: uppercase;">Get Mastered</h4>
                </a>

            </div>

            <div class="collapse navbar-collapse navbar-left navbar-main-collapse col-lg-9">
      <ul class="nav navbar-nav" style="text-align: left; padding-left: 0px;">
            <!-- <li><a href="/services/mix-feedback/">Mix Feedback</a></li> -->
            <li><a href="/#mastering">Mastering</a></li>
            <!-- <li><a href="/services/audio-cleanup/">Audio Cleanup</a></li> -->

        <li><a href="/#reviews">Reviews</a></li>
        <li><a href="/our-work/">Our Work</a></li>
        <!-- <li class="active"><a href="/blog/">Blog</a></li> -->
        <li><a href="/login/">Login</a></li>
      </ul>


            </div>
        
        </div>
    </nav>

    <script>$("div.navbar-fixed-top").autoHidingNavbar('showOnUpscroll',false)</script>
    


    



<div style="padding: 0px; margin: 0px;">
  <div class="col-lg-3 sidebar hidden-md hidden-sm hidden-xs" style="padding: 0px; margin: 0px;">
    <!-- data-spy="affix" -->
    <div  data-offset-top="0" style="padding: 0px 30px 0px 0px; margin-top: 75px;">
    <div>
      <!DOCTYPE html>


<section class="bg-white" style="padding: 40px 50px 5px 60px; margin-top: -9px; text-align: right;" data-spy="affix">


	<h5 class="sidebar-head">
	    <a href="/blog/about/What-Is-Get-Mastered/" class="sidebar-text">What is Get Mastered?</a><br>
	</h5>

	<br><h5 class="sidebar-head">
	<a href="/blog/about/Who-We-Are/" class="sidebar-text">Who Are we?</a><br><br>
	    </h5>

	    <h5 class="sidebar-head">
	<a href="/blog/about/What-Is-Mastering/" class="sidebar-text">What is mastering?</a><br><br>
	    </h5>

	<!-- <h5 style="color: #777; display: inline; text-transform: uppercase;">Learn about</h5>


	<p style="color: #999; font-size: 15px; padding: 5px 0px 0px 0px;">

	
	    <a style="color: #ccc;" href="/directory/sound/">
	        sound
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/automation/">
	        automation
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/compression/">
	        compression
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/EQ/">
	        EQ
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/reverb/">
	        reverb
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/depth/">
	        depth
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/mixing/">
	        mixing
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/mastering/">
	        mastering
	        
	        <br>
	        
	    </a>
	
	    <a style="color: #ccc;" href="/directory/mixing,/">
	        mixing,
	        
	    </a>
	

	</p> -->
	<!-- 
	<h5 class="sidebar-head">Search the blog<br><br></h5> -->


	<!-- <h5 class="sidebar-head">
	    <a href="/#about">Try us out for free</a><br><br>
	</h5> -->

</section>
    </div>

    </div>
  </div>
  <div class="col-lg-6" style="padding-top: 0px; padding-left: 20px; padding-right: 20px;">

    <div class="row-fluid bg-white" style="padding: 35px 0px;">
    </div>


    <div class="row-fluid bg-white">
      <br>
      <div>
      <div>
<h2>That term appears in:</h2>
</div>
<div id="results" class="row-fluid" style="padding: 8px 0px 700px 0px;">
	
</div>

<script>
	var JEKYLL_POSTS = [];
	
	JEKYLL_POSTS.push({
		title: "Motivated to Mix",
		link: "/blog/Motivated-to-Mix/",
		content: "The more and more I mix, the more I realize that the more I mix the more mixes I am proud of!I know I know—sorry. Too much alliteration for one intro. But it’s true. Mixing more projects at once and putting yourself on a schedule will drastically increase the quality of your mixes, as long as you do it right.Give yourself too much workI find a huge benefit to throwing away the traditional idea of a deadline for music. Musical inspiration—whether you are producing beats or writing a folk guitar part—comes in waves, depending on your mood, the time of day, and what you had for breakfast. The music that we write and mix naturally stems from the context of your life, and forcing a musical work to completion before you are naturally inspired to produce more usually results in a song you are not proud of.Instead, I find that having many projects going on at once greatly stimulates my inspiration. Maybe I can’t focus enough to finish writing that cinematic strings part today, but hey, I would love to mix that other song I am writing! Giving yourself a multitude of different songs to work on—including mixing songs that you didn’t have a hand in writing—maximizes the time that you spend writing music. And since no one song has a set deadline, you are free to table things when they are not working out.Set a scheduleEven though you should abandon the traditional idea of a deadline for individual works, you should still set overall deadlines and timeframes for more abstract goals. Set a goal for yourself that doesn’t depend on any individual song coming to completion, such as writing an album in a year, or releasing one polished, mastered song a month. Goals like this—that don’t depend on specific songs—will greatly increase the efficiency of your daily and weekly writing, because you will focus on what matters, and what is closest to being finished. Setting a timeline like this also keeps old projects from getting too old.Quit more oftenBut what about that one song with that you can’t quite finish? The one with a great hook that you can’t seem to write a melody for? Counterintuitively, often the best thing you can do for these kinds of songs is put them of the backburner. Indefinitely. In other words: quit! Focus your time on another song that you know you can finish, or a task you know you are feeling ready to do. One day, you might open up that old, cobwebbed folder and find that you know how to finish it at long last. Above all else, prioritize what you know you can finish, and put everything else away for a little bit.Keeping ContextNot only will this method get you completing more projects, moving from work to work daily or weekly will really help smooth away the biases you develop listening to one song over and over. If you have been mixing the same track, or hearing the same melody, over and over, the obvious points of concern for the track become worn down, and you will start to focus on elements that really, really don’t matter. As someone who has recorded hundreds of vocal tracks before realizing that, maybe, this is an instrumental track, you can take it from me!RecapHere is a quick recap of this writing method  Keep your overall goals (e.g. mix one song per month, etc) front and center. If you can meet that goal today, prioritize it.  Start building a backlog of unfinished songs  if you can’t add to an old project, start a new project  keep your creativity on its toes by jumping from song to song and not focusing too hard on one mixAnd if you need that extra kick of motivation to make yourself finish at least one song a month, we have just the plan for that. Having a mastering engineer reviewing your work every month, and committing to completing one song your are proud of a month, will really kick your productivity and abilities into overdrive!"
	}); 
    
	JEKYLL_POSTS.push({
		title: "Treat your life, then your control room",
		link: "/blog/treat-your-life/",
		content: "Why is the girl in this stock photo making this face?Is it because she just got paid $30 to wear headphones in a stock photo shoot? No— it’s because she’s listening to your newest mix, and she’s really really into it. Take another look: "Straight fire" she whispers before fainting on the spotBut why does she love your mix so much? Is it your new preamps, or the thirty-plus hours you spent penciling in de-breath-ing automation on all of your harmony vocal tracks? No way - she really doesn’t care about any of that.She cares about how the mix sounds, to her. Not how the mix sounds to someone who has heard the underlying song countless times, and who has been through the entire recording and writing process. If this girl thought your mix was a little too high-end heavy, you’d be unwise to discard her opinions as uninformed, regardless of whether or not you agree (especially, in this case, since she is probably younger than you, and can hear more high end than you by more than a bit).Get more, and better, ears on your workWhat matters to me more than anything in terms of personal improvement is getting feedback on my work. At the end of the day, you want your mix to sound unequivocally great to anyone who listens - so it makes sense to get as many ears on your mix as possible. This may seem obvious, but truly take a second to think about how often you ask for feedback - even I don’t think that I can receive enough!Of course, however, quality trumps quantity. The best thing your mixing skills can benefit from, more-so than any amount of room treatment, high quality DAC’s, or expensive microphones, is an engaging, reliable group of friends with whom you can regularly and mutually grow your skills.Finding people as passionate as you are about mixing is an extremely rewarding experience, and you’ll find that you start thinking completely differently while mixing. Nowadays when I mix, I will sit down in front of my DAW, take a deep breath, and press play from the beginning, pretending like I have a peer in the room. Even when I don’t have people directly critiquing me, knowing that they will hear my mix later in the week during a weekly critiquing session makes me think completely differently, as though they are in the room next to me. I start focusing on bigger issues, like overall levels, and suddenly smaller things, like all that de-breath-ing, don’t matter at all (maybe those harmony vocals were too loud anyway!)I find that my mixing skills grow much more rapidly when I can rely on people to critically evaluate my work, and listen with different ears than mine, because I always bring myself back to the bigger issues at hand. It’s only when I go too far down the rabbit hole that I’m dissatisfied with my mixes - I often find a simple solution myself or am offered one by a peer when mixing this way.Start a listening group today with some peers - don’t put it off! Pretty soon you’ll see your mixes improve drastically, and you’ll be able to focus on the bigger issues at hand. And you’ll be able to impress this woman. "What a great mix. I can't help but smile!"Thanks for reading! I would love to hear how your experiences mixing with friends has gone in the comments."
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Mastering?",
		link: "/blog/about/What-Is-Mastering/",
		content: "Music mastering means different things to different people. Of course, there are some technical aspects of mastering that most agree upon - but to us at Get Mastered, there are a few hugely important parts of the mastering process that are lost with cheap mastering services and auto-“mastering” softwares.Mastering is, in essence, the process of transforming a mix which has been bounced or exported as one stereo audio file into a finalized, ready to release song.A well mastered song will sound  cohesive,  professional,  warm,  great in any format,  great through any sound system,  great in any space,  and loud,while still retaining the overall style, vision, and dynamics of the original mix. This requires a vast amount of technical know-how as well as objective, experienced ears.Our mastering process at Get Mastered includes 6 important steps:Step 1 - Getting to know YOUStep 2 - FeedbackStep 3 - CohesionStep 4 - TranslationStep 5 - Loudness!Step 6 - ReviewStep 1 - Getting to know YOUIn music, everyone has their own style and tastes. Most mastering services don’t take this into account, but our engineers at Get Mastered truly work hard to understand you as a musician or producer.During this stage, we ask you to send us music you consider beautifully produced, so that we can get an idea of what you are working towards. We also ask you to send us some of your previous musical works, so we can get a sense of your own style - which is something we never want to compromise!Step 2 - FeedbackIn any profession, feedback is extremely important, and music production is no different. Independent music production is rapidly growing, and with it a loss of feedback loops between multiple engineers working together.For our next step, you send us your best mix, and we work with you to bring it to your quality standards, if it is not there already. This is a vital step to mastering that is completely lost with cheap services and auto-“mastering” software. There are some things that are truly impossible to fix in the mastering stage without compromising other areas of the mix, and it is extremely important we iron out any mix-level issues before mastering.Through this step, our clients are also able to improve their own mixing skills, and learn straight from the experts!Step 3 - CohesionNext, our engineers take the reigns.Often times, before mastering, a song can sound more like a “mix” than a “song”. During this stage of the mastering process, we perform extremely precise, high quality analog-emulated processing to gel the mix together without sacrificing its overall sound. We don’t just slap on a preset with a mastering suite plug-in - every mix we work on has different needs, and we treat each song only with the processing it needs.This stage is where the magic happens, so to speak - afterwards, your song will sound crisp, warm, and cohesive.Step 4 - TranslationThe next step is making sure your music sound great everywhere, or making it translate well to different  environments. Sure, your mix sounds good in the studio - but how does it sound in a car? Over the radio? As an MP3? On Spotify? On vinyl? On headphones? On cheap laptop speakers?This is a vital stage of mastering that is - again - completely lost with cheap services and auto-“mastering” software… noticing a theme here?Step 5 - Loudness!Finally, after your song is sounding amazing, we make it loud without sacrificing dynamics.We don’t just slap a limiter on the output and call it a day - it is actually extremely difficult to increase the apparent volume of a song without sacrificing dynamics, warmth, and low-end punchiness, and there is a lot of nuance involved in this stage of our mastering. The track you receive from us will ultimately be as loud as you would like us to make it!Step 6 - ReviewOnce all of our engineers sign off on how the master sounds, we send it back to you for review. We tweak everything to your liking, to make sure you are getting the sound your music deserves. Our work is done only when you are completely and fully satisfied with how your music sounds! We care about our clients, and don’t want you leaving with anything until it blows you away.At Get Mastered, our mission is simple: give you the best sounding music possible. We know how to do it, and we can’t wait to show you. Try us out today, and see how your music can sound when you Get Mastered!"
	}); 
    
	JEKYLL_POSTS.push({
		title: "Writing With Mixing In Mind",
		link: "/blog/Writing-With-Mixing-In-Mind/",
		content: "I like to think of writing a song like making a puzzle, and mixing a song like trying to actually put together the pieces of that puzzle. When I’m recording a composition I was not involved in creating from the start, it’s often a more challenging puzzle than if I’m making the pieces from scratch myself, and I can dictate how they fit together from the start.In this post, which I received some requests for, I’ll walk through how to create those puzzle pieces yourself, using music theory as a guide. You’ll see that once you have the means to write parts that fit snuggly with each other, mixing and recording become much easier and sound completely natural!Please note: This post will take a little bit of music theory to get the most out of. At some point, I would love to have a guest on here to give a quick theory primer; for now, I’ve linked to some great music theory introductions on the bottom of this post.Building your puzzle piecesThere are plenty of little tricks you can use during your writing process to make recording and mixing a breeze, and most of them boil down to writing different instrumental parts to fill different roles.It helps to think of things spatially on a piano. Say we are writing a song with a vocalist, a guitarist, a synth player, and a drummer. We’re going to make a shell of a song using just that information. Keep in mind that each of these instruments in this example can be substituted with any other instrument you normally use to write.I always write new parts by putting them into one of two categories (usually with some light overlap):  Parts that use a new section of the keyboard  Parts that reinforce a section of the keyboard that is already in useVocals (or, any melodic instrument)So with that in mind, let’s say I’m starting the song with a vocal melody, for example, that spans this range on the piano:Do re... You can see that my vocal part in this case ranges from B2 to E4, which is generally my most frequented area melodically as a baritone singer. This range works well for my voice - I’m not trying to push it to the limit and write a part that strains my voice and what it can naturally accomplish. It’s great to start with vocals for this reason, since vocal ranges are more variable than other instruments across different players (as opposed to piano, for instance, which can hit the same notes no matter who is playing it)Vocals are also unique in the sense that every head of every vocalist is different, and thus every vocalist has notes which resonate acoustically in their mouth, throat, and nasal cavity better than others, and are thus easier and more natural to sing. This combination of range and resonances often leads to certain keys being preferred by vocalists (I lean towards F, Bb and Ab, personally). In this case, we’ll pretend we’ve tried or had our vocalist try different keys, and we’ve picked the key of E major for this song.Guitar (or, any chordal instrument)The easiest next step in writing the song for me, since I’m also a guitar player, would be to add a guitar part. Since the vocals are going to be the main focus of the song in this case, I’ll want the guitar to sit behind them, and reinforce them. I’ll also want to fill out the keyboard a little bit more, so I’ll write a part that fills out the high-mids a bit (knowing that I have synth to fill out the lows). Notice how I’m already using terms like “fill out the high-mids” during the writing process, rather than the mixing and recording process!Here is the range of my guitar part in green, with my vocal range still in blue:mi fa...You’ll notice here, I do have some overlap (indicated by the red arrow). It’s difficult to achieve complete isolation between parts in practice. Here I’m trying to illustrate that this style of writing often has overlap; as long as it’s only a few fleeting notes, this kind of overlap is negligible - we’re concerned with the region about which most of the notes in a part are clustered. Of course, avoiding overlap entirely  will almost always sound good in comparison!Guitar players may have noticed this, but it’s worth pointing out: this range (B3 to E5) would consist of chords spanning only three or four strings. That’s right! More often than not, it sounds great to leave unused notes for others to take, especially when those notes are better suited for other instruments. I very regularly omit the bottom note of the chord from my guitar part so that the bass player can have that note for themselves.  With any sort of mid-range instrument, I usually use a lot of chord inversions and root note omissions.In contrast, for some styles, like singer-songwriter and folk, full open chords are often played on guitar, since there are usually no other instruments to fill that role, and acoustic guitar is more often than not a star player in those style recordings, and can take a more central role in the song.Synth (or, any bass instrument)Next, we need to fill in those bass notes! Enter: the bass synth.so la...I’ve noted the bass synth’s range in magenta.You’ll notice this time that we have a gap between the lower vocal part and higher bass synth part, indicated by the red arrow. Don’t panic! When in doubt, leave more space between parts - better to have more room you can fill in later if need be (say, if the low-mids need beefing up when you listen to a rough mix of your song)Now we’ve used all of our non-percussive instruments. But wait - there’s still a ton of empty space at the top of the piano! Do we need to fill that in?Of course, we could fill that in with another instrument if we wanted. If I do this, I usually save it for the chorus sections of the song, to add some impact.However, I often find that I leave this region empty. The reason that this isn’t as noticeable as, say, a missing bass region part, is that all of the instruments we’ve added have higher frequency harmonics. These harmonics will fill in our higher frequencies when we mix - I’ll even usually find that when I’m mixing, there is too much competition up there, and I’ll have to chop out some high frequency harmonics of, say, the bass and vocal parts, to leave room for the guitar harmonics.Sometimes, I’ll even just add a non-pitched, higher frequency part, such as shaker, white noise, or cymbal crashes, to the chorus of songs. This adds some excitement and differentiates these sections from the rest of the song.Drums (or, any percussion instrument)Now, so far, our three instruments, vocals, guitar, and bass synth, have fallen into the “new section of the keyboard” category, to fill certain roles (from bullet 1 above: “Parts that use a new section of the keyboard”). Now that most of our keyboard is filled up, we can add percussion as a supporting player to beef up our song with a backbone (which fits bullet 2 from above: “Parts that reinforce a section of the keyboard that is already in use”)Here is a depiction of our percussion instruments, in orange, layered underneath all our other parts:ti do!I’ve noted the percussion as orange blocks because every drum, whether you care to think about it or not, has a fundamental frequency (so long as it’s been tuned!). I try to tune my drums to the key of the song when I can, as this really locks the backbone of the song into place, and sounds much more cohesive. Here are some tips for tuning acoustic drums - if you’re using electronic drums with a pitch control built in, you really have no excuse to leave them untuned!In this case, since our song is in E, I’ve tuned the kick drum (the lowest orange block) to a low E1, the fundamental of the track. Then, I’ve tuned two toms (the next two blocks, moving right) to a B1 and an E2, since B is the fifth in the key of E. My snare (the fourth block) is also a B, B2. Roots and fifths are great for drums, since you can use both for minor and major keys, and they are part of the arpeggio of the key. Remember, though: this is just an example - you can place your drums anywhere you’d like!I’ve also included an arbitrarily placed fifth and sixth orange block to denote a high hat and a crash cymbal. These may not sound pitched exactly, but they are up there to indicate that these will both be adding higher frequency content, and supporting some of the harmonics from our other instruments.This drum part is overlapping almost all of our other instrument regions! But that’s okay, because as most drum parts are, this will be a supporting part, rhythmically accenting different notes of the vocal melody, guitar chords, or bass synth, to glue the different regions together with a rhythmic backbone.Pick up the pieces"Sorry Grandma, rain check on the puzzles... I have to write music"Now we have a song that will practically mix itself! We’ve created our own puzzle pieces by partitioning our keyboard to different players, and it’s fairly straightforward to see how we’ll approach recording and mixing this song. Here are some examples of things I could do now (key word could!):  Record vocals with a dynamic microphone, since we want to emphasize the low-mids rather than compete with the guitar by using a condenser microphone  Record the guitar with our amp EQ’d with low amounts of bass, and perhaps a little more than average amount of treble  Add some high-passed, bright reverb to the track to fill in the high end of the songEtc, etc, etc. Once you’ve written a song this way, mixing and recording become easier, because you have a pre-defined frequency region for each instrument to use as a jumping off point. You can also easily keep writing and recording new parts if the song needs more beefing up, by adding more supporting players, such as some subtle piano chords, guitar doubling, or harmony vocals where appropriate.This is a rather extreme example of partitioning the keyboard while writing; there’s nothing to stop you from writing three bass parts in a song, for example, but it’s certainly rare! Even in these extreme examples, however, you find that the two overlapping instruments work together and don’t fight each other.Remember, as with all things musical, these are just helpful guidelines. Try writing some new songs like this, and then break some rules and see how it sounds!Great music theory referencesHere are some great music theory references:Great primer from Sound on Sound that pairs well with this postFree online class from Berklee (can’t beat that!)A more thorough and pedagogical guideI’d love to hear your experiences writing with this technique in the comments. Next, I’m planning a post about how to use your music theory knowledge until it becomes second nature, so stay tuned!"
	}); 
    
	JEKYLL_POSTS.push({
		title: "Who We Are",
		link: "/blog/about/Who-We-Are/",
		content: "As mastering engineers, we are passionate about anything and everything audio. Read our bios below to learn more about our diverse professional backgrounds, experiences, and why we love what we do!Our Engineers:Keenan Hye - Founder and Mastering EngineerMy name is Keenan, and I’m currently a candidate for a Masters of Engineering in Acoustics at Penn State University. I am originally from Madison, Connecticut, and I have been producing music since I was in early high school. I earned my undergraduate degree in Electrical Engineering and Music Recording, and have years of practical experience under my belt.During the past five years, I have had experience researching human perception of audio quality, creating automated room equalization algorithms, and engineering directional hearing devices to aid people with hearing loss. All of this experience has given me a vast and varied understanding of topics important to mastering and our perception of music, including psychoacoustics, room acoustics, and digital audio processing.To satiate my right brain, I spend most of my down time writing original music and lyrics, and, of course, recording and mixing my own music as well. I grew up playing saxophone, but now I’m primarily a singer and guitarist.Helping people make great audio is absolutely my passion, and to me there is no substitute for the human touch that a real mastering engineer can add. I take pride in producing professional audio at prices that are much easier to swallow than most other professional mastering services, yet which still allow myself and other engineers to make a living helping artists that want to enhance their audio production skills.If you would like to learn more about me, my personal website can be found here.Connor Eichinger - Mastering EngineerHailing from Pleasantville, New York, Connor is pursuing a degree in Music Industry with a minor in Recording. He plans to graduate in 2017. I brought Connor on to the Get Mastered team recently, as I was incredibly impressed by his natural talent for mixing and mastering, and his visible drive to produce the best finished project possible, no matter how long it takes.Connor has years of experience working with audio in various contexts, including producing and recording, mixing and mastering, sound design, and film post production. He has composed and performed acousmatic music in a quadraphonic setting as well as through live sound diffusion with Harvard University’s Hydra loudspeaker orchestra. Connor has also composed music for commercials and short films, and is currently finalizing a 5 track EP which will be his debut as a solo artist.Lee Schuna - Mastering EngineerProducer name: Analogue SelfFrom New Richmond, Wisconsin, Lee has been learning the art of recording since age 13. Starting out recording covers of songs for YouTube, he eventually started writing his own music and recording all the parts in his parent’s basement. Working with the limitations of lo-fi equipment and the old Intro version of Ableton, learning to creatively solve audio problems became hugely important to his process.An alumni of Northeastern University’s Music Industry program, Lee spent time working in both campus recording studios, as a full time house engineer during the inaugural semester of the Digital Media Commons Studio. There, he worked with a variety of clients and taught workshops on a variety of topics.After a 3 year long battle with tendinitis that prevented him from playing most instruments, Lee focused on electronic music production, and has since been very involved in making in techno, ambient, and music for film and stage. Sound design has been Lee’s primary interest after working on an award winning film for Campus Movie Fest and getting involved in theatrical sound design. He’s been able to use the skills he learned from recording and mastering audio in other contexts to sculpt sound with attention to detail."
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Get Mastered?",
		link: "/blog/about/What-Is-Get-Mastered/",
		content: "My name is Keenan, and for all of my life I have had a passion for really, really great sounding recordings. I grew up recording my own original music and have never looked back - I’m currently a candidate for a Masters of Engineering in Acoustics and received my undergraduate degree in Electrical Engineering and Music Recording. I’ve had experience researching human perception of audio quality, creating automated room equalization algorithms, and engineering directional hearing devices to aid people with hearing loss. I’m of course also an active musician, singing and playing guitar in a band, and messing around with whatever instruments I can get my hands on.Having self-produced my own music for many years, I know how difficult it can be to perform, record, mix, and master something that you are completely proud of. In my experience, there was always that special “extra something” missing. Before long I realized an easy way to make my audio better: get someone with more experience to help!I had heard of mastering being the stage where this “extra something” is added by professionals. Unfortunately, typical mastering services were completely beyond my budget, while any cheaper services or automated softwares I could find were not cutting it at all. Even more than that though, I wanted someone who knew what they were doing to listen to my music and not only give my track a professional sound, but give me feedback on what I could be doing better. And I couldn’t find that outside of my college classrooms.That is why I created Get Mastered: as a way for musicians, recording hobbyists, and other audio professionals to foster a direct relationship with an experienced mastering engineer, and not only create amazing recordings, but learn how to become better producers in the process.Here’s to making amazing music, together!"
	}); 
    
	JEKYLL_POSTS.push({
		title: "Preparing Your Mix For Mastering",
		link: "/blog/Preparing-Your-Mix/",
		content: "So, you’re ready to Get Mastered! The following are steps you should make sure to take before you send me your mix:1) Turn off any limiters or other plugins on the output bus2) Turn down the master fader -6dB3) Bounce in a high quality, lossless format4) Turn off any normalization or dithering while bouncingMore details on each of these steps can be found below:1) Turn off any limiters or other plugins on the output busI cannot stress the importance of this first step! If you have inserted any plugins on the output bus (also known as the master track) of your mix, please, please bypass them before you bounce the mix for mastering. A lot of the tricks I use to beef of the sonic quality of your mix rely on a certain level of dynamic range; if the mix is already limited, I can’t do much to remove that on my end, and I won’t be able to bring your song to its full potential.Here is an example of a song that will be very difficult for me to improve, because it has been heavily limited, and has little to no dynamic range (that is, there are not many ups and downs in the audio):In this specific example, the limiter actually added tons of distortion to the track that I could not remove! Bad bad bad!!!Now, here is an example of what your track should look like after turning off any output limiters:Beautiful, beautiful dynamics.It is important to also turn off other plugins on the output bus, such as compressors, for me to do the best job possible. “But Keenan,” you might shout, “the mix just isn’t the same without them!” If you are staunchly opposed to bypassing some output plugins, let’s compromise: send me one bounce with output plugins enabled, and one with them disabled.2) Turn down the master fader at least -6dBThe reasoning for this step is similar to step 1: I need to have a little extra space on top to do what I need to do. Please do me a favor and leave me 6dB of headroom by turning your master fader down -6dB before bouncing your mix.3) Bounce or export your mix in a high quality, lossless formatYour mix should be bounced in the same format in which it was recorded (generally, 24 bits with a sample rate of no less than 44.1kHz). If you recorded at a lesser quality, don’t sweat it.File formats should be lossless (that is to say, not mp3s!!!). Lossless file formats include .wav, .aiff, and .flac.4) Turn of any normalization or dithering when bouncing your mixNormalization will boost the volume of the track, destroying all those 6dB’s of headroom! Dithering is not necessary if you are sending me a high quality, lossless mix, as you should be.If you have any more questions on preparing your mixes for mastering, please don’t hesitate to contact me at services@getmastered.com"
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Mastering?",
		link: "/blog/Get-Started-Part-4/",
		content: "Contents:###What is Mastering?Finally, we have recorded and mixed an entire song! Now we can move on to the final part of producing a song: mastering.Mastering involves adjusting the entire track to fit commercial standards in terms of loudness and digital format. It would be a shame, for instance, if your songs weren’t featured on a radio station because they are at an inconsistent volume when compared to typical commercial releases.Mastering is also sometimes thought of as simply making a mix “better.” This can involve altering the tone of a track, adding a sense of cohesion to the track, or accentuating certain instruments over others. This is the trickiest part of mastering; how do we alter certain aspects of a song without access to the individual tracks?In this final section of Get Started, we will start with the more technical aspects of mastering, specifically increasing loudness and changing a song’s digital format. Then we will discuss ways through which we might be able to make a mix just sound “better”.This section of Get Started will also heavily rely on information we learned in parts one through three of the series, so make sure you know your vocab (you can always quickly run back and check words you forgot about!)####LoudnessThe most tell-tale sign that a song hasn’t been mastered is when that song simply doesn’t sound as loud as other songs when played at the same volume.You might think, “Well, that’s easy to fix: just turn up the output level of the track!” Unfortunately, it’s not quite that easy. We can only increase the volume of a track so much before we start clipping on our output. So how else can we make things sound louder if we can’t increase volume? To be able to fix this problem, we need to think about what defines loudness.Remember from Part 1 of this series, when we discussed loudness in terms of our brain’s translation of the strength of air molecules vibrating in the air? It turns out that our brain has a funny way of interpreting these vibrations that has to do with the average volume and the peak volume of a sound.Imagine a normal snare drum hit: there is a loud attack and a short low level resonance that dies pretty quickly.Now, if we compress that snare drum, we are increasing the average volume of the entire sound. We can go even more extreme and limit the snare using a limiter (a compressor with an infinite ratio). This is essentially how mastering engineers make tracks sound louder: very careful limiting.When we limit an entire track, we are basically squeezing the loudest sounds down in volume, and raising the level of the whole track, effectively increasing the lower levels in the mix (and therefore the average volume of the track). Essentially, we are trying to lessen the gap between our loudest sounds and quietest sounds.This gap is called a track’s dynamic range. Usually it is helpful for a mastering engineer to work with a track with a lot of dynamic range; other wise there is nothing left to limit!###A Final Note: Should I Master My Own Music?Of course you can master your own music. But should you?A lot of people say no. A large added bonus of the mastering process is a second pair of ears on your mix, in particular a set of really experienced ears.Unfortunately though, mastering is often expensive. It is oft said of mastering that “you get what you pay for,” and experienced mix engineers know not to skimp on a budget mastering job. However, though it may be all well and good to get a second pair of ears on a mix, sometimes it is just financially unaffordable.On the flip side, if you have a debut EP, you don’t want that to be your first master ever - you will most likely end up compromising all of the hard work you put into writing, recording, and mixing the songs!Here at Get Mastered, we have our own special version of the typical mastering process that focusses on collaboration with artists and engineers. If you would like to learn more about our mastering, read our post here"
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Mixing?",
		link: "/blog/Get-Started-Part-3/",
		content: "Contents:###What is Mixing?Now that we’ve learned all of the ins and outs of digital audio, we can get to the fun stuff: mixing and mastering. In this part of the Get Started series, we will talk about the mixing process.Mixing, in it’s simplest definition, is altering the volume of sounds, and adding them all together. Think of a sound engineer at a live show: they are often being yelled at to “turn up the vocals” or “turn down the electric guitar” or “turn down the electric guitar even more please.” Turning different instruments “up and down,” often called changing the levels of each instrument, is the most basic form of mixing.Mixing has become more advanced throughout history, as a result of inventions such as equalization, stereo music, artificial reverberation, and other effects. Due to these advances, modern mixing is also concerned with three other important characteristics of individual sounds within a song: tone, placement in “virtual space,” and effects.Often in a DAW, we will have various tools we can apply to a track, software we call plugins. In this article, we will also go through some of the tools used to achieve different sounds within a song. In addition to plugins, we often have other DAW controls available, including some for sound editing.In this part of Get Started, we will discuss the four fundamental aspects of mixing (levels, tone, placement, and effects) and the tools used to craft them in greater detail. We will also go through some of the basic DAW controls, including editing.###LevelsThe most basic of the four aspects of mixing is levels. The level of a track within a mix is simply the volume of that track.Typically, in a DAW, each track has what’s called a fader, which is a slider that dictates the volume of the track. A bunch of faders from Logic X are pictured below:The faders are those grey sliders placed vertically on each track.Levels are important in their own right, as their relation to each other often define the hierarchy of instruments in the mix. Levels also play an important part of simulating a sound coming from a far distance, as we will see later.A typical situation that arises during mixing is that a certain instrument may need to be louder or quieter depending on the part of the song. For example, maybe the kick drum need a little boost during the chorus. Another typical situation is that certain parts of the instrument’s sound may need to be louder or quieter than other parts of the instruments sound. For example, maybe the kick drum is too loud riiiight when the batter head hits the drum head, but is a little too quiet when it is resonating.(as a little side note: I will always be saying  kick drum and not bass drum, since the latter terminology is sometimes confusing when used in the same discussion as bass guitar, which happens quite frequently).The two different types of situations call for two different solutions. In our first situation, we can solve the problem with what is called automation, while in the second situation, we can solve the problem with what is called compression. In the next two sections, we will briefly go over these two tool in our mixing arsenal.####AutomationAutomation is used to change the value of a paramter throughout a song. In this case, we can use automation to change the level of a certain track in different sections of a song, or even during certain words, as is common with vocal tracks.Here is an arbitrary example of automation on a kick drum track.As you can see, as the song plays from left to right, the level of the track will change from -5.9 dB, to +1.0 dB, to other values.In most DAWs, you have the ability to draw in different values over the course of a song with a pencil tool (producing shapes known as envelopes). This is what I did to quickly make the automation points in the above picture.Most DAWs also have the functionality to record parameter changes made by the mix engineer as the song plays. When levels in particular are changed during the course of a song by an engineer monitoring the mix, it is typically called riding the fader, for pretty obvious reasongs.Sometimes, though, we have changes we want to make to a sounds’ level that are too fast for us to catch with our hands or a pencil tool, or changes that keep repeating every snare hit, say. It would be very difficult or tedious to use automations to fix these sorts of problems, so for those we turn to compression.####CompressionCompression is a tool we can use to “even out” certain parts of a sound. An easy example for this is our kick drum example, where we might want to dull down the attack, where by attack we mean beginning of the sound.Here’s a compressor that has a nice visualization of some common controls:This is a Waves compressor that I don't love by any means, but is a great visual depiction of how compression worksAll of the bolded vocab to follow can be found in the second left-most column of this particular compressor (which even has more controls than we will touch upon here).Compressors work by scaling down the level of a track as it gets louder. So if a compressor had what’s often called a ratio control set to “2:1”, it would compress the signal down so it only ever gets half as loud as it normally would. Similarly, a 3:1 ratio would squash the sound a third of it’s normal volume. You get the idea: the higher the ratio, the more compressed the sound.Compressors also often have a threshhold control, which controls the point at which the compressor starts working. If, say, we wanted only the sound spike that happens during the attack of the kick drum to be compressed, and everything underneat that to remain the same, we would set our threshhold between the spike or peak value, but above the resonance of the kick drum after it has been hit.The x-axis (horizontal axis) of our pictured compressor represents the input volume of the compressor during any given moment, and the y-axis represents the output volume. This compressor has a ratio of more than 20:1, which is visualized clearly by the virtually flat output above its theshhold of -40 dB.You can also tell from this graph that the input can go from -100 dB to 0 dB, while the output can never go above -40 dB. These ranges are called the dynamic range of a track. the reason why a compressor is named as it is is because it is compressing the dynamic range.Sometimes we want to bring the compressed track back up in volume, since compression often makes our track sound quieter. We can do this by adding makeup gain. If we give this compressor 40 dB of makeup gain, our graph looks like this:Now, our dynamic range is the same size, it has just moved from -100 dB through -40 dB to -60 dB through 0 dB. This, in effect, makes our track sound louder than it originally did, because it will never fall below -60 dB (as opposed to our input sound, which still has the dynamic range of -100 dB through 0 dB). Some compressors automatically apply makeup gain, so these compressors, counterintuitively, automatically make a sound louder, because they are both compressing and adding makeup gain.Compressors often also have attack and release controls. Imagine the compressor is “squeezing” the sound, as a person might squeeze a stress ball. The attack control dictates how quickly the compressor “squeezes” the sound, and the release control dictates how quickly the compressor “let’s go” of the sound.Compressors are a very tricky thing to hear, especially when they are used subtley, as they ofoten are (it should be noted that a 20:1 ratio as seen is the example above is an exageration of typical values; usually we will have a ratio between 1:2 and 1:5). A huge part of mixing and mastering is grasping exactly how changing these controls changes a sound, and a compressor is one of your two most important tools as a mixing engineer.####Limiters, Expanders, and Multi-Band CompressorsCompressors also come in a few different flavors. A standard compressor acts as we have discribed above, but their are other types of compressors with slight differences.A limiter for instance is a compressor with ratio and threshhold controls tied together: as the threshhold decreases, the ratio increases. A popular limiter is shown below:An expander is the opposite of a compressor; it boosts the sound above a certain threshhold, essentially a compressor with a ratio between 0 and 1.There are also multi-band compressors, which are compressors that work differently depending on the frequency of the sound. A de-esser is a type of multi-band compressor tailored to work on frequencies sqpecific to sybilant sounds produced by human speech (think of an “s” sound).###Tone and EQIn addition to compression, equalization (EQ), a tool with which you can change the tone of an instrument, is one of the most important tools with mixing and mastering.EQ is used to turn up and down certain frequencies, and often uses several user-defined filters to do so (remember filters?). These filters can be different shapes, which will be outlined in the following sections.The two most basic, and proabably most used filters, are the high pass filter (HPF) and low pass filter (LPF). As their name might suggest, they each pass certain frequencies through them untouched while filtering out other frequencies. A HPF filters out the highs, while a LPF filters out the low. Confusingly, these filters are also sometimes called low cut and high cut filters, respectively. Just take a moment to think about what the terms mean, and this alternate terminology is not too confusing.HPFs and LPFs have a parameter called slope (sometimes called roll-off) which dictates how steep the filters are. They also have a cutoff frequency control which ditates where they start passing highs or lows.Below is a picture of a HPF, with a slope of 24 dB/octave and a cutoff frquency of 79 Hz:And next is a picture of a LPF, with a slope of 24 dB/octave and a cutoff frquency of 7 kHz:In both of these visualizations, we can see our EQ “greying out” (i.e. filtering) frequencies below and above their cutoff frequencies (respectively).Another type of filter is a bell filter, which is named because of it’s bell shape. Bell filters can be turned up or down with a gain control, and also often have a Q control, which dictates how broad the filter is. Bell filters usually have a lower Q value, meaning they are broader in nature. Bell filters are often used gently, and are usually quite subtle.Below is a picture of an EQ using a bell filter with a gain of +9 dB and a Q value of 0.3 (located below the gain value in this picture). It is located at the frequency 1040 Hz:As you can see, with a low Q value, we get a broad, bell shape. In this case, as opposed to the HPF and LPF, the grey region of frequencies are boosted.Alternatively, if a filter has a high Q value, meaning it is very narrow, and is being used to cut (i.e. the gain value is negative), we usually call this a notch figure, aptly named because we are notching out a certain small band of frequencies. Usually a notch filter can be used effectively to cut out a particular frequency in a very transparent way.Below is a notch filter at the same frequency, with a gain of -24 dB and a Q value of 2.1:In this case, again, the grey frequencies are cut.Since a filter with such a high Q value is rarely used to boost frequencies, we don’t really have a name for that case. Call it a “generally bad sounding filter”  if you would like.As should be clear, most changes made to a sound via EQ are subtle or transparent in nature, though they can be  more heavy handed; it all depends on the context and desired result! However, sometimes more radical tonal changes are desired, and for those we often resort to effects, which we will discuss later.###PlacementWhen we listen to well produced music in a quiet listening environment, it often sounds like we, the listener, have been placed inside of an actual space, a room where we can hear, for example, a piano to our left, a tambourine to our right and far away, and a singer front and center, sounding very close.Placing sounds within this virtual environment is one of the jobs good mix engineers will spend a lot of time and thought on. Often, this can give an okay mix a serious wow-factor, and (of course) is quite difficult to do effectively. An ability to recreate a realistic virtual environment is often what separates the good mix engineers from the great.The placement of an instrument within the virtual space of a mix is based off of two factors: the panning of the instrument, or its angle from the listener (left, right, center, and anywhere in between), and the depth of the instrument, or how far away the instrument sounds.Usually, a good mix engineer will create a hierarchy of instruments within a mix, having the main parts be front, center, and close, with the supporting parts being panned harder and sound as if they are coming from farther away. Things like lead vocals, kick drum, snare drum, and bass will typically be front, center, and close, while maybe a supporting harmony vocal, hi hat, or guitar might be skewed to one side, and far back. Again though: this all depends on the individual song! These are just some enormously common occurences, not hard and fast rules.####ReverbPanning is usually a rather simple to control; most DAWs have a built in panning knob on each track. Depth however, isn’t quite that easy, but there are a few tools at our disposal. When creating a sense of depth, mix engineers often employ, in conjunction with other tricks, a tool known as reverberation, or reverb for short.Reverberation in a room is any sound wave you hear that does not come directly from the source of that sound. Often in addition to the direct sound wave, we will hear sound waves that started at the source, bounced off of a wall, or two, or three, or thirteen, and then arrived at your ear. All this extra sound is what we call reverb (sometimes thought of as echo, but there is a difference. Reverb is often much more difficult to discern compared to reverb. Technically, acoustic echo is a certain type of reverb).Reverb can be added artificially in most DAWs using anything from basic to hugely computational algorithms, and often the quality is dicernable to more experienced ears. Real reverb can also be recorded directly, often with great results; think of the sense of depth you get listening to a well done live recording of a concert.Here is an example of a great reverb engine that comes bundled with Logic X:A reverb plugin, like most effects plugins, has a wet and a dry fader (located on the top right of this picture), used to control the reverb (the wet signal) and the original, direct sound (the dry signal). In addition to this, reverbs have some other common controls including pre-delay, which sets a time delay between the wet a dry signals.We won’t get too much more into how to go about creating a sense of space in your mix here: I have to save some of those details for future series! (which, have I mentioned, you will miss out on if you don’t sign up for the mailing list in the sidebar?).###EffectsNow finally, everything else: effects. Almost any change in sound that is not as subtle as the above techniques can be classified as an effect (at least, that is my definition).Any of the tools we talked about earlier (automation, compression, EQ, and reverb) can be used as effects if they are heavy handed or radical enough. More common effects include delay, distortion, chorus, flanging, and pitch shifting, but we won’t go into them much here (again, we’ll go into them in future series if you’re not missing out on those!). They’re the kind of things that most of you have tinkered with, and if you haven’t, they’re loads of fun, and usually come prepackaged with most DAWs. Go turn some knobs an listen to what happens!###DAW ControlsIn addition to tools that alter sound, a lot of DAWS have added controls for ease of workflow. A couple of these you may have noticed in the earlier picture of the Logic X faders, notably the buttons labeled M and SThese stand for mute and solo, respectively. Muting a track does exactly what you would expect, and can be used to completely remove the track from the mix. Soloing a track temporarily mutes all other tracks, so that a mix engineer can listen closely to just the track being solo’d. This can be very useful for fine tuning individual sounds within a mix.####EditingSometimes a mix engineer might want to align certain tracks, remove squeaks, breaths, or thuds, or pick and choose the best parts from multiple takes. All of these features are luckilly included in most DAWs as editing controls.Usually this can be done in what is often called the workspace of the DAW, pictured below:In the workspace, we can see a layout of particular regions (chunks of recorded audio) within a song. Each horizontal row represents one track, often containing color coded regions of audio for different sections of a song. These regions can be moved around, if, say, two drum tracks aren’t quite lined up because of some recording delay between the two tracks.Regions can also be cut, meaning the region is sliced into two separate regions. This is often done by cutting at the playhead, which is the white vertical line in the above picture which indicates where in the song we are. If we hit play, the song will start playing wherever the playhead lies, and will follow along through the song.Editing is often a manual process, and is helpful for fixing things that we can’t fix with plugins.###The Finished MixFinally, we have our levels set, our tone sounding nice, our mix has a sense of space, we’ve added some tasteful effects, and we’ve edited out all of our extraneous noises. Now what?Once a mix is finalized,  it is bounced (exported) to a single audio file. Usually, however, this single file can sound a bit quieter than songs we normally listen to, and may not sound as cohesive as we may like. Luckilly, we can fix these problems through mastering, which we will talk about in the final part of the Get Started Series"
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Digital Audio?",
		link: "/blog/Get-Started-Part-2/",
		content: "Contents:  The Microphone  The Analog to Digital Converter          Analog vs Digital      Audio Quality      The Digital Waveform      Sample Rate (“Digital Time”)      Bit Depth (“Digital Space”)      Clipping        The ComputerWe live in a crazy time of human existence where we can recreate almost any sound around us, with basically no effort. Most smartphones come preloaded with a voice memo app, giving everyone the power to record sound and play it back at a moment’s notice.Why should you care about the technical details of digital audio? Well… you want your music to sound better than this, don’t you?Learning only a little bit of the technical know-how surrounding audio will put you miles ahead of your fellow musicians, and open your ears to so much more going on underneath the surface of the music you listen to every day.Sound waves go through an obstacle course of sorts as they make their way inside a computer in a process known as digital recording. In this article, we will follow sound energy through this obstacle course, which we call a signal chain, as it makes its way into a computer. Here is a block diagram with the three basic parts in the recording signal chain:The infamous signal chainAs you can see, we start with the acoustic sound we described in Part 1 of this series, and end with digital audio inside your computer. There are three main sections to the recording signal chain: the microphone, the analog to digital converter (or the ADC), and of course the computer.Let’s start with the bridge between the acoustic world and electrical worldThe MicrophoneMicrophones are a type of transducer, which is a device that shifts energy between different forms. In the specific case of the microphone, acoustic energy is shifted into electrical energy, so they are categorized as Electro-acoustic transducers. We won’t go too into detail about how different microphones work, as there a few different methods by which they can convert energy between the electrical and acoustical domains. Essentially, you can think of it this way: a sound wave pushes on something, which in turn pushes on electrons, sending electrons out of the microphone in the exact same pattern that the microphone was pushed by the sound wave.Here is a ubiquitous microphone, Shure’s SM57:Perhaps the most ubiquitous microphone in existence today, Shure's SM57Sound waves come through the metal mesh in the front end (called a grille), and electrical waves come out of the back of it.Remember how a sound wave is just air molecules vibrating back and forth? This is a bit of an over-simplification, but when pushed by this vibrating air, the microphone essentially pushes electrons out it’s back end such that they vibrate at the same frequency of the imposing sound wave. This is done, in the case of this microphone, through a vibrating electromagnet.So if you could “hear” electrons, and put your electron-hearing-ear on the output of the microphone, you would hear all of that stuff from part one: fundamental frequencies, harmonics, loudness, etc. A piano’s timbre looks almost the same coming into and going out of the microphone, it is just that on the input air molecules are making this shape, and on the output electrons are making this shape.Different microphones have different characteristics (sometimes called color) due to the distinct mechanisms through which they convert energy. These audible characteristics stem from the fact that microphones do not pick up all frequencies of sound equally; take for example the SM57 above. Included by the manufacturer with this microphone is something called a frequency response, which is exactly what it sounds like: a depiction of how the microphone responds to certain frequencies (and thus, how it colors the sound). You can see boosts and cuts of certain frequencies in the response below:The frequency response of Shure's SM57, or, how loud or quiet each frequency will become when recorded through this particular microphoneWe can see from the frequency response of the SM57 that it has a relatively flat response in the mid-range, with some boosts in the high end, and a cut in the low end that gently slopes as we get lower. Thus, microphones, just like any part of the signal path, act as a filter. Though filtering in microphones is subtle, it is always there, whether you like it or not.There are also other types of transducers we can use in the place of a microphone for different types of sounds: for example, if you are recording electric guitar, you can use the pickup of the guitar, which is another type of transducer. Or, if you are recording something like a synthesizer, which outputs electronic signals only and no actual sound, you can bypass this whole stage, since you are already starting with electrons! Remember: In this first step, our goal is only to convert sound energy to electrical energy.The Analog to Digital ConverterNext, the signal travels to a soundcard, a mixer, or, what you are probably using to record sound: an audio interface. Each of these has something called an Analog to Digital Converter, or an ADC, which acts as a translator for your computer, which works in 0’s in 1’s, not electrons. The ADC basically says, “Okay, electron here, electron there… that means 110010”.The ADC is a complicated device, but is unrefutably the most important one to digital recording. Don’t be intimidated! We will split this part of the signal chain up a little, and explain the ADC more in depth. A lot of concepts explained for the ADC have wide-arcing implications in modern recorded music, so it is important to really understand how this thing works.Buckle your seatbelts!Analog vs DigitalAnalog to digital conversion is hugely important, and not something you want to mess up. To understand a bit more of how this works, we first need to understand what analog and digital mean in the first place.The electrons entering the ADC from the cable exist in the “real world.” This is different from the 1’s and 0’s which exist in the “digital world”. You know this; if you are watching a live video stream of a panda, the panda is not the “real world” panda, but it is a “digital world” copy that looks just about close enough, as far as we can tell. There is no panda inside your computer (really! Go ahead and check). We would call the panda inside your computer the “digital panda” and the panda in The Smithsonian’s National Zoo the “analog panda.”When you think analog, think “real world”. Think: there is no smallest chunk.In the “real world” we can keep dividing space forever. If you zoomed in on the real world panda, you could zoom down to its individual hairs, and then further to the proteins making up the hair, and then down to its atoms, and even further; endlessly in fact, if you have a big enough microscope. There is no smallest chunk.However, if you zoomed in on the digital world panda, you would be left with individual pixels, each with the same size, past which you cannot zoom in. In this case, each pixel represents what we call a discrete chunk of space, meaning a finite amount of space.In the “real world,” we can also keep dividing time, forever. A second is pretty small, but a millisecond is even smaller, and a microsecond even smaller than that. If we keep dividing time, there is no point at which we hit a limit. There is no smallest chunk.However, if you slowed down the panda cam, which is digital, you would be left with individual frames, clicking along at a certain speed. In the digital world, these clicks represent a discrete chunk of time (again, a finite amount of time.) There is nothing that exists between these framesSo remember: in the digital world, there is always a smallest chunk, and we call these chunks discrete values.Audio QualitySpace and time are two aspects of the “real world” that the digital world absolutely cannot perfectly recreate, and this turns out to be crucially important in recreating sound waves, which can get pretty darn fast, and pretty darn small, and on which you could zoom in, forever, both in space and time. In the digital world, we need a smallest, otherwise our computers would not be able to handle the size of the audio file. We need a discrete chunk of electrons to be our smallest chunk. Luckily, there is a point below which our ears can’t hear the difference between real world audio and digital audio if this discrete chunk is small enough. Generally, the smaller you go, the higher the quality of the recording. In audio, we call each discrete chunk a sample. Thus, the more samples you have in any given time or space, the higher the objective quality of your audio.(A note: this is the same language we use for things like drum samples; sample just means a chunk of audio. For this discussion, however, when we use the word sample, we are refering to the smallest chunk. In fact, a drum sample as we normally think of it is actually made up of many, many smaller samples. This is not meant to confuse; only to clarify that the sample we are refering to is the smallest sample)The Digital WaveformThinking about audio as a wave, as we have before, can be very helpful in understanding these two ideas. Plus, audio is usually displayed as a waveform in recording softwares. Take the following digital waveform of a clap I clapped:A clap - in digital!This is quite intuitive to understand, but worth explaining: At the points where the waveform is very tall, it means that the ADC recieved a big chunk of electrons, and therefore that the microphone sent a big chunk of electrons down the cable to the ADC, and therefore that the microphone recieved a big chunk of air molecules. In other words: something loud just happened! At the points, on the other hand, where the waveform is centered at the 0, there is no sound, as far as the ADC can tell (even though absolutely zero sound is not possible anywhere but inside a vacuum, since there is no air in a vacuum, hence the famous “in space, no one can hear you scream”)Intuitively, we understand that the length of these waveforms represents “digital time.” Again, if we zoom in lengthwise, the thinking is the same as zooming in time on our panda cam: we know there is a limit to our time resolution here as well. If we were to zoom in enough, there will be a point at which we could see the wave click along sample by sample, just as we saw the panda cam click by frame by frame.Changes in height of the waveform are what we can think of “digital space.” If we think back to our panda cam, the thinking is the same: we know there is a limit to this resolution. If we were to zoom in enough, there will be a point at which we could see discrete jumps in height of the audio waveform, just as we saw discrete pixels on the panda cam.Let’s dig a little bit deeper into these ideas of “digital time” and “digital space,” and learn some better vocabulary for those terms:Sample Rate (“Digital Time”)The ADC can’t translate electrons to binary at an infinite speed; it has to go back and forth, just as you would imagine a human interpreter going back and forth between worlds in which different languages are spoken. Except this human translator goes really, really, fast, on the order of tens of thousands of times per second.The speed at which the ADC can translate analog waves to digital waves is what we call its sample rate. If you don’t have a fast enough sample rate, you start to miss those rapidly vibrating high frequencies; you start to get less total samples, and the quality of your music suffers.Now, you would think, intuitively, that to record a given frequency, you would at least have to record at the sample rate corresponding to that frequency (to record a 300 Hz tone, for example, you would have to at least record at 300 samples per second.) However, if we put ourselves in the shoes of the ADC, we realize rather quickly why this doesn’t quite work out.Imagine you are assigned a job as an ADC, and you had to figure out the lowest sample rate you could employ to capture a 300 Hz wave.If you tried capturing a 300 Hz wave by sampling at 300 samples per second, you would capture the values represented by the blue line on the graph below.As you can see, the blue line, or what we captured as an ADC, never moves from 0! We have failed to capture our 300 Hz tone; we cannot see 300 Hz with a sampling frequency of 300 Hz. As far as we know, as an ADC, there is no 300 Hz wave.Let’s try again, but this time doubling the sampling frequency. Notice, we get the exact same thing:However, as we will see, any frequency above 600 Hz we will start getting values greater than zero. Here is 601 Hz:You can just start to see nonzero values appearing if you look at the values highlighted by red dots.Let’s try 1200 Hz for a more clear picture:And 2400 Hz:And finally let’s take a ton of samples; here is 44100 Hz:Where our blue digital signal is completely overlayed on our green analog signal. As you can see, any sample rate above what we call the nyquist rate, which is double the rate of the highest frequency wave we are trying to capture, will give us nonzero values, and the higher we go, the better of a picture we get of said wave. Even in the case of the 601 Hz wave, we have enough nonzero points that, if we tried to sketch a sine wave that intercepted all of them, we would have a sine wave at 300 Hz. In fact, computer software is so good at making these sketches, 601 Hz will do just fine to capture a 300 Hz wave. The higher our sample rate, the higher the maximum frequency we can capture.It turns out, 44100 Hz (or 44.1kHz), the sampling frequency we used in the last example, is a typical sampling frequency, one that is probably used by the ADC inside of your audio interface. Why is this used? Remember from part 1: humans can only hear from 20 Hz to 20 kHz. So if we wanted to be able to capture a maximum frequency of 20 kHz, we would need a sample rate above the nyquist rate, which is in this case 40 kHz.44.1 kHz is well above the 40 kHz, so we can use this sample rate to record anything we can hear without error. The specifics of the number are a bit more “just because.” The music industry could have picked any frequency above 40 kHz, but chose 44.1 kHz as a standard, just because.Bit Depth (“Digital Space”)We know that if we zoomed into the panda cam, we would eventually reach pixels of uniform height and width. So it makes sense that if we zoomed into a waveform, we would find a similar smallest chunk. In audio, we call this smallest chunk a bit. Before we move on, a quick explanation of the binary system is in order:The binary system is just a system of counting that is easy for computers to use. We humans use a system of counting that goes something like “0, 1, 2, 3, 4, 5, 6, 7, 8, 9”. For any value above that, we shift over a decimal place and start again, “10, 11, 12, 13, 14… 21, 21, 23, 24… 31, 32, 33, 34… etc”. You know how it goes from there (I hope). This is called a base-ten, or decimal counting system, because we have ten different symbols that we use in representing any given value.In contrast to this system, computers use a base-two, or binary counting system, which only uses two symbols: 0 and 1. A computer counts “0, 1” and then shift over a decimal place and start again: “10, 11, shift, 100, 101, 110, 111, shift, 1000, 1001… etc”. These are just different ways of representing the same values: 0 in binary is 0 in decimal, 1 in binary is 1 in decimal, 10 in binary is 2 in decimal, 11 in binary is 3 in decimal, 100 in binary is 4 in decimal, and so on, forever (for more on conversion information, see wikipedia).Each digit in a base-two number, at least for a digital system like a computer, is called a bit, and it turns out that the number of different values we can represent in a binary number is directly related to how many bits (how many digits) it contains: specifically by raising 2 to the number of bits. So if we had a 4-bit number, we would have 2^4 = 8 values we can represent. We can count these ourselves: 000, 001, 010, 011, 100, 101, 110, 111, which, remember, represent 0 through 7 in decimal (note that here we are using leading zeros, which are zeros on the left hand side meant to be placeholders. These are meaningless, just as they are meaningless in base-ten, e.g. when you are writing a date: 04/01/2015 which we understand as 4/1/2015)We need the height of our waveform to be a binary number, since we want to use it inside of a computer. So the smallest possible chunk of height is 1 bit of value 0. The second smallest chunk of height is 1 bit of value 1. Notice we cannot have a value in between 0 and 1; since we are in the digital world, and we are making a bit our smallest chunk, we have to either round up to 1 or down to 0.Say we had a 1-bit ADC. At any given point in time, this ADC looks at it’s input (coming from the microphone) and says, “how many electrons do I have here?”. Unfortunately, our ADC can only count to 1 (since 2^1 = 2 values, which, remember, represent 0 and 1 in binary). So if we know we might see, at a maximum (and this is hypothetical), 60,000 electrons, we can work within the rules of: :  if I see between 0 and 30,000 electrons, that means 0  and if I see between 30,001 and 60,000 electrons, that means 1Which is not all that useful. If we were trying to capture a sine wave, it would look something like:The analog wave we are sampling (green) is a sine wave at 300 Hz, while our sampled wave (blue) doesn’t quite look like a sine wave.Now say we had a 2-bit ADC. At any given point in time, this ADC looks at it’s input (coming from the microphone) and says, “how many electrons do I have here?”. Now, our ADC can count to 3 (since 2^2 = 4 values, which, remember, represent 0, 1, 10, and 11 in binary). So if we know we might see, at a maximum (and this is hypothetical), 60,000 electrons, we can work within the rules of:  if I see 0 electrons, that means 00  if I see between 1 and 20,000 electrons, that means 01  if I see between 20,001 and 40,000 electrons, that means 10  if I see between 40,001 and 60,000 electrons, that means 11Now we are getting somewhere a little closer to our analog wave:(Note that here we are using the base-ten values on the y-axis, not binary)Imagine now we have a 16-bit ADC, or, if we want to learn some vocab, an ADC with a bit-depth of 16 bits. Now, our ADC can count to 2^16 = 65536. Now our resolution is much better; we can represent each individual electron with a bit of it’s own, and even have 5536 bits left to spare. Here is what we get when we sample at that bit-depth:  Finally, our blue sampled wave completely matches our green analog wave (in fact, it completely overlaps it in this picture!)Most ADC’s in use today are either 16-bit or 24-bit, though they often do not get down to resolution as fine as one electron, or anywhere close. It turns out one electron’s worth of sound from a microphone is extremely, if not impossibly difficult to hear in most realistic scenarios. In fact, ADC actually measure electrical power, which is voltage times current, but thinking about things in terms of electrons is much simpler, and will suffice for our discussions.Clippingblah blah clippingThe ComputerFinally, we have a digital audio wave from our ADC that we can save onto our computer! To store these digital sound waves, computers typically use a user-friendly software called a Digital Audio Workstation, or DAW for short. There are many different types of DAWs, ranging from basic freewares such as Audacity to more advanced programs like Logic, Pro Tools, and Cubase.(An important thing to note moving forward is that I primarilly use Logic X, because I work on a Mac, and my experience with Logic has been better for my personal needs than alternative DAWs. But don’t run away if you don’t have any intention of using Logic, or don’t use a Mac! I will jump through flaming hoops to make sure everything I talk about on Get Mastered is applicable accross DAWs. If there is demand for it, I can make some more Logic specific tutorials down the line)Moving on to Part 3, we will work exclusively inside of the computer. It is important to remember that since we are working inside of a computer from now on, everything is digital. If you skimmed over those big analog vs. digital concepts, it will come back to bite you later when you are dealing with some more advanced features of DAWs; make sure you understand these concepts! They are tricky, but incredibly important, and your understanding of them will put your music production skills miles ahead of 99% of musicians poking their noses around Garage Band."
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is sound, really though?",
		link: "/blog/Get-Started-Part-1/",
		content: "Contents:  What is sound?  Fundamental Frequency, Harmonics, and Pitch          An Example: The synthesizer        Tone  Filters  Loudness  Recreating SoundWelcome to the Get Started mini-series! In this series, we will review the basic scientific foundations of the modern music production process in an easy to understand format. If you’re looking to enhance your scientific understanding of the recording process and be a cut above the rest, you’re in the right place.The series will be split into four sections:Part 1: What is Sound, Really Though? An overview of sound and it’s basic characteristics (hint: you are reading this one)Part 2: What is Digital Audio? How sound waves, electricity, and computers work together (jump to Part 2)Part 3: What is Mixing? An overview of the modern recording process, including tools used in altering recorded music (jump to Part 3)Part 4: What is Mastering? A bit on what the term “mastering” means to different people (jump to Part 4)So let’s get started by answering the question:What is sound?The definition of sound is a little more complicated than it would seem. Sound is our perception of a medium’s variation of pressure versus time through our ears, and the auditory processing parts of our brain. Typically, this medium will be air, though - and I hope I don’t sound crazy - we can perceive variations in pressure of other media too, like water! Just fill up your tub, stick your head underwater, and listen for your roommates to ask you what on earth you are doing. You will be able to hear them, though maybe not as clearly as you would in air.For now, we are just going to deal with air, since most of us aren’t listening to music underwater (most of us).To better understand what this definition means, we need to dip our toes (just our toes!) into physics (only for a second!). Remember: DON’T PANIC. Physics is more afraid of you than you are of it.Pressure is a distribution of force across a given area. In terms of sound, the force comes from air molecules bouncing against the area of your eardrum. The more air molecules you have shoved into one space (i.e. the higher the density of air molecules), the more force they can exert against your eardrum. Conversely, the less air molecules you have shoved into one space (i.e. the lower the density of air molecules), the less force they can exert against your eardrum. It is this variation in air molecule density, that leads to a variation in pressure against our eardrum that we hear as sound.Below is a great animation with which we can visualize the change in density of air molecules that leads to our hearing of sound. Notice how if you pick one position in the square, there is a constantly changing density of air molecules, as the higher pressure “ripples” of air seem to move through that point.Sound!In fact, a very useful way of thinking about these “ripples” of higher density air molecules is to call them waves. Specifically, we call these ripples sound waves, and we can think about them just as we could a wave created by dropping a rock into a pond.It is a little bit easier to think of these variations in pressure as vibration. Think of when you play music too loudly in your car and the whole thing starts vibrating; that vibration is the direct effect of sound.An easy way to differentiate sound waves from one another is to measure the distance between the points of highest pressure, or the points of highest air molecule density. This is pretty easy to see: All the “darker” circles are high pressure points. Since we are talking about sound waves, we call this distance the wavelength, the length of the wave. Every wavelength, the wave repeats itself. A wavelength is a distance per cycle of the wave.Now, keep your eyes focused on one individual air molecule in the animation above (one individual dot). Notice that it is vibrating back and forth at a constant rate (if you are having trouble seeing it, it’s a little easier to see towards the center of the image). Now pick any other dot, and notice that it too is vibrating back and forth at the exact same rate. Pull out a stopwatch if you don’t believe me.This vibration rate is called a wave’s frequency. Air can vibrate quickly, and air can vibrate slowly.  When air vibrates quickly, it has a high frequency, and when it vibrates slowly, it has a low frequency.The unit of frequency is Hertz (abbreviated Hz), which simply means cycles per second. If you did pull out your stopwatch, and noticed that each dot was returning to the same position once every second, it would have a frequency of 1 Hz (one cycle per second). If a sound is oscillating at 100 Hz, that means that air molecules are moving back and forth 100 times every second.If air vibrates at a frequency anywhere between 20 Hz and 20,000 Hz, we can hear it as sound. Outside of that range, the air is still moving; we just can’t hear it! Other animals can hear different ranges of frequencies that we cannot (dogs, for instance, can hear frequencies above 20,000 Hz)Remember, a wavelength is a distance per cycle, and a frequency is a cycle per second. If a wave goes 340 meters per cycle (has a wavelength of 340 meters) and cycles once every one second (has a frequency of 1 Hz), it is moving 340 meters every second (this is just about the speed of sound in air at sea level). Thus, sound waves also have a speed, or velocity that is a direct relationship between wavelength and frequency.Fundamental Frequency, Harmonics, and PitchIn nearly all real life scenarios, sound will not consist of one frequency, but of many, many different frequencies layered upon each other. The lowest of these frequencies is called the fundamental frequency of a sound. Every other frequency within the sound is called a harmonic.Our brains translate these fundamental frequencies into something we call pitch. When you hit a middle C on a piano, you hear a pitch corresponding to that key’s fundamental frequency, which is, in this case, 261.6 Hz. This is the lowest frequency in the sound you hear when you hear a middle C on a piano, but there are still harmonics above this, which are translated by your brain as timbre, or the characteristic of the sound. If you listened to a trumpet playing concert middle C, you are still hearing the fundamental frequency of 261.6 Hz as its pitch, but the trumpet’s harmonics, and thus its timbre, are significantly different from that of a piano.So remember: pitch is your brain’s translation of fundamental frequency1, and timbre is your brain’s translation of harmonics.An Example: The synthesizerPerhaps the easiest instrument on which to visualize fundamental frequencies versus harmonic frequencies is the synthesizer. Most synthesizers start by sending out very basic tones that can then be altered with all of those knobs, buttons, and sliders. These basic tones are usually include easy to recognize shapes, such as sine waves, square waves, triangle waves, and sawtooth waves, represented below (in that order):A sine wave (red), a square wave (green), a triangle wave (blue), and a sawtooth wave (purple magenta crossbreed)It’s easy to see where these names come from!Something you might notice is that, though these waves have different shapes, they have the same fundamental frequency; they each repeat 3 times in this picture (if this picture’s length represented 1 second, they would be oscillating at 3 Hz). So, saying the same thing a different way, these all have the same pitch.The difference in the shapes of these waves represents the different timbres you hear listening to these sounds. Just like we talked about with the piano and trumpet, these all have the same fundamental frequencies, but different harmonics.A sine wave has no additional harmonics. If we play a sine wave at 1000 Hz (or 1 kiloHertz, often abbreviated 1 kHz), we hear this.A square wave consists of a sine wave at its fundamental frequency, plus sine waves at odd-order harmonics. In other words, the fundamental (1000 Hz), the fundamental times 3 (3000 Hz), the fundamental times 5 (5000 Hz), the fundamental times 7 (7000 Hz), all the way through infinity. That sounds like thisSimilarly, triangle and sawtooth waves consist of similar series of adding together different harmonic patterns of sine waves. In fact, every sound ever heard in the history of forever consists of different combinations of sine waves, added together and sometimes variable in time. In these simple cases, there are patterns we can follow, but for more complex sounds like that of a piano and trumpet, the sequence of harmonics fall out of any pattern and become much more complex, and thus are very difficult to recreate synthetically. That is why if you set a synthesizer to a piano or trumpet setting, you will usually get something that sounds similar to a piano or trumpet, but is easily distinguishable from the original sound.ToneThings start to get tricky, from a vocabulary perspective, when we try to separate timbre and tone. The definitions I am giving are by no means universally agreed upon, but will suffice for the sake of music production.Whereas timbre is the typical collection of harmonics allowing you to distinguish different instruments, tone is a description of the flavor of that timbrel source. Think of different timbres as a specific kinds of food (e.g. french fries vs. pumpkin pie) and tone as certain flavorings of one type of food (salted french fries vs. unsalted french fries).An electric guitar might be bright (lots of high frequency harmonics), distorted (lots of added unnatural harmonics), or warm (lots of pleasant sounding harmonics), for example, but it still has the timbre of an electric guitar. Brightness, distortion, and warmth are some examples of descriptors of tone; there are countless more that are often based more in musical slang than in science (e.g. muddy, clean, thick, crunchy)FiltersSomething cool us humans have figured out is that you can filter sounds. A filter is something that changes the strength of certain frequencies in a sound (and also usually the phase, or the timing of certain frequencies in a sound relative to one another). Just as you might filter your coffee, you are, with a filter, removing parts of a sound, effectively “straining out” certain frequencies. If you think a grand piano has too many high-frequency harmonics, you can filter some out by closing the lid. When you stuck your head underwater earlier, the water was filtering out some of the higher harmonics of your roommates’ voices.What’s more, air itself can act as a filter, if you get enough of it! If you have been to an outdoor concert, or seen a marching band, you’ll notice that the further away you move from the sound source, the more air filters out high frequencies, while you can still hear lower frequency sounds like the bass guitar or bass drum at about the same volume (though usually quite a bit muckier).In all of these cases, a physical object acts as a filter. Though all of these examples acted upon high frequency harmonics, filters can act on any range of frequencies. We tricky humans have figured out how to manipulate audio so that we can eliminate or accentuate certain frequencies in a much more exact way than by filtering sound through physical objects. When done electrically, filtering is called equalization, which we will come back to in Part 3 of Get Started.LoudnessWhen we filter a sound, we are changing the loudness of certain frequencies within that sound. So it makes sense to go back and ask the seemingly innocuous question: what is loudness?Loudness is our brains translation2 of the strength of a sound, or, in other words, how hard the air molecules which make up a sound are vibrating. Intuitively, the harder the air is a-shakin’, the louder the sound is a-soundin’, and conversely, the weaker the quieter. A sound’s total loudness relates to the loudness of all of it’s component frequencies: not just the fundamental frequency, and not just the harmonics. We hear different frequencies with different sensitivity, which is why loudness is often different than absolute volume, and why certain songs often sound louder than others even though they are scientifically the same average volume.Recreating SoundOf course,  you wouldn’t be reading this now if recorded music didn’t exist! So how to we make it sound like there is a group of musicians living inside of our computer speakers, ready to play any song at our will?The trick lies in converting those wiggling air molecules into wiggling electrons. Just like music in the air consists of different frequencies of air molecule vibrations, music in your computer consists of different frequencies of electron vibrations.How does your computer do this? Continue on to Part 2: Digital Audio to find out            note I keep saying brain’s translation here; your brain is not always exact - in fact, studies have shown your brain doesn’t need a fundamental frequency to perceive pitch. This is outside the scope of this series, but I will come back to it some day. Just like most things concerning psychoacoustics, it’s pretty trippy, and just a little too confusing for now. &#8617;              see above &#8617;      "
	}); 
    
</script>
<script src="/js/search.js"></script>
<script>
	new jekyllSearch({
		selector: "#results",
		properties: ["title", "content"]
	});
</script>
      </div>
    </div>
  </div>

    </div>
  </div>
  <div class="col-lg-3 sidebar hidden-md hidden-sm hidden-xs" style="padding: 60px 0px; margin: 0px;">
     <!-- include sidebar-right.html  -->
  </div>
</div>


    


    </div>

    <div class="col-lg-12 bg-gray">
      <div class="row-fluid" style="text-align: center; padding: 3% 5%;">

        <h3 style="color: #777; display: inline; font-size: 16px;">
    Get Social
</h3>

<br>
<br>
<a target="_blank" href="http://www.facebook.com/getmastered" style="padding: 0% 1.5%;"> <img class="social" src="/img/social-media/Button-FB-trans-g.png" width="25px" height="25px"> </a>

<!-- <a target="_blank" href="http://www.instagram.com/getmastereddeals" style="padding: 0% 1.5%;"> <img class="social" src="/img/social-media/Button-Insta-trans-g.png" width="22px" height="22px"> </a> -->

<a target="_blank" href="http://www.soundcloud.com/getmastered" style="padding: 0% 1.5%;"> <img class="social" src="/img/social-media/Button-SC-trans-g.png" width="45px" height="22px"> </a>

<a target="_blank" href="http://www.twitter.com/getmastered" style="padding: 0% 1.5%;"> <img class="social" src="/img/social-media/Button-Twit-trans-g.png" width="25px" height="25px"> </a>

<br>
<br>
<p style="font-size: 14px; color: #aaa;"><b><a href="/contact/">contact</a> - <a href="/blog/about/What-Is-Get-Mastered/">about</a> - <a href="/blog/">blog</a> - <a href="/#faq">faq</a></b></p>


      </div>


    </div>

    <!-- Core JavaScript Files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/jquery.easing.min.js"></script>    
    <script src="/js/jquery.scrollTo.js"></script>
    <script src="/js/wow.min.js"></script>
    <!-- Custom Theme JavaScript -->
    <script src="/js/custom.js"></script>

  </body>
  
<!-- Google Tag Manager  -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MSTFPS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MSTFPS');</script>
<!-- End Google Tag Manager -->

<!-- Facebook bull -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>


</html>

