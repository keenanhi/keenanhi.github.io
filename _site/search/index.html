<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Get Mastered</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mix and master your own music">
    <meta name="author" content="Keenan Hye">

    <link rel="shortcut icon" href="http://i1045.photobucket.com/albums/b459/keenanhi/Flavicon_zps31q8d1uz.png" />

    <!-- Bootstrap Core CSS -->
    <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css">

    <!-- Fonts -->
    <link href="/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
      <link href="/css/animate.css" rel="stylesheet" />
    <!-- Squad theme CSS -->
    <link href="/css/style.css" rel="stylesheet">
      <link href="/color/default.css" rel="stylesheet">   

  </head>

<body id="page-top" data-spy="scroll" data-target=".navbar-custom" class="bg-white">
    <!-- Preloader -->
    <div id="preloader">
      <div id="load"></div>
    </div>

    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation" style="background: #000; padding: 1px 0;">
        <div class="container-fluid">
            <div class="navbar-left page-scroll col-lg-3" style="padding: 0px;">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="hidden-xs hidden-sm hidden-md" href="/" align="right">
                    <!-- <h2> The DIY Mastering Blog</h2> -->
                    <h4 class="master-color-lighter" style="padding: 15px 20px 5px 0px; margin: 0px;">Get Mastered</h4>
                </a>

                <a class="hidden-lg hidden-xl" href="/" align="left">
                    <!-- <h2> The DIY Mastering Blog</h2> -->
                    <h4 class="master-color-lighter" style="padding: 15px 20px 5px 0px; margin: 0px;">Get Mastered</h4>
                </a>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-left navbar-main-collapse col-lg-9">
      <ul class="nav navbar-nav" style="text-align: left; padding-left: 0px;">
        <li class="active"><a href="/">Home</a></li>
        <li><a href="/free-master">Free Master</a></li>
        <li><a href="/mastering-services/">Mastering Services</a></li>
        <li><a href="/archive/">Post Archive</a></li>
      </ul>
            </div>
            <!-- /.navbar-collapse -->
        
        </div>
        <!-- /.container -->
    </nav>
    


    



<div style="padding: 40px; margin: 0px;">
    </div>

<div class="row equal bg-white" style="padding: 0px; margin: 0px;">
  <div class="col-lg-3 sidebar bg-white hidden-md hidden-sm hidden-xs" style="padding: 0px; margin: 0px;">
    <div data-spy="affix" data-offset-top="0">
    <div>
      <!DOCTYPE html>


<section class="bg-white" style="padding: 0px 15px 5px 30px; margin: 0px; text-align: right;">

<h4 style="color: #777; display: inline;">
    What is Get Mastered?
</h4>

<p style="color: #999; font-size: 15px; padding-top: 5px;">
Get Mastered is a resource for musicians to hone their recording and mixing skills, and home of the absolute best value mastering services -- 

<span><a href="/about/What-Is-Get-Mastered/" style="color: #aaa;">Learn more</a><br><br></p>

<h4 style="color: #777; display: inline;">
    Who Am I?</h4>

<p style="color: #999; font-size: 15px; padding-top: 5px;">
My name is Keenan and I've been mastering, mixing, and recording music for six years. I currently aspire to gain a masters in acoustics<br><br></p>


<h4 style="color: #777; display: inline;">
    Done mixing?
</h4>
<p> <a style="font-size: 15px;" href="/mastering-services/">Get Mastered, right now</a></p>

<br>

<h4 style="color: #777; display: inline;">Looking for something?</h4>


<p style="color: #999; font-size: 15px; padding-top: 5px;">


    <a style="color: #ccc;" href="/directory/sound/">
        sound 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/analog-vs-digital/">
        analog-vs-digital 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/microphones/">
        microphones 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/automation/">
        automation 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/compression/">
        compression 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/EQ/">
        EQ 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/reverb/">
        reverb 
        
        	|
        
    </a>

    <a style="color: #ccc;" href="/directory/depth/">
        depth 
        
    </a>


</p>

 <form action="/search" method="GET" style="padding: 10px 0px 10px 20px;">
 		<!-- <label style="font-size: 17px;" for="query">Search:  </label> --> 
              <input style="font-size: 13px;" type="text" name="query" placeholder="Or search a term">
            </form>

<!-- <p style="color: #000;"> <a style="font-size: 15px; color:#999;" href="/archive/index.html">Full Archive</a></p> -->

</section>
    </div>
    <div class="row-fluid bg-white" style="padding: 0px; margin: 0px;">
      <!DOCTYPE html>

<div id="mc_embed_signup">
<div class="row-fluid" style="margin: 0px 10px 30px 0px; padding: 0px;">
<form id="contact-form" method="post" action="//getmastered.us10.list-manage.com/subscribe/post?u=bb14eb93467fa9f89bc914f09&amp;id=1d85fcd133" style="padding: 0px 0px;" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <h4 style="color: #777; text-align: right; margin: 0px; padding: 5px;">Want 10% off your first job?</h4>
  <!--   <p style="font-size: 14px; color: #777; text-align: right; margin: 0px; padding: 5px;">Sign up for the mailing list and get exclusive discounts:</p> -->
    
    <div id="mc_embed_signup_scroll" style="padding: 10px 0px 10px 30px;">
                <div class="container-fluid">
                        <div class="row">
                            <div class="form-group">
                                <textarea class="form-control" name="EMAIL" id="mce-EMAIL" rows="1" cols="100" required="required" placeholder="Enter your email to get exclusive discounts"></textarea>
                            </div>
                            <div id="mce-responses" class="clear">
                    <div class="response" id="mce-error-response" style="display:none"></div>
                    <div class="response" id="mce-success-response" style="display:none"></div>
                </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
                            
                            <button type="submit" name="subscribe" class="btn btn-skin pull-right bg-dark" value="Subscribe" id="mc-embedded-subscribe">Submit</button>
                     </div>
            </div>
        </div>
</form>
</div>
</div>

    </div>
    </div>
  </div>
  <div class="col-lg-9 bg-white" style="padding-top: 0px; padding-left: 20px; padding-right: 20px;">
    <div class="row-fluid" style="color: #000;">
      <div>
<h2>That term appears in:</h2>
</div>
<div id="results" class="row-fluid" style="padding: 8px 0px 700px 0px;">
	
</div>

<script>
	var JEKYLL_POSTS = [];
	
	JEKYLL_POSTS.push({
		title: "What is Mixing?",
		link: "/mini-series/getting-started/Get-Started-Part-3/",
		content: "A quick note to the reader: This site is currently under construction, though I am still accepting submissions for a free master and taking clientsContents:  What is Mixing?  Levels          Automation      Compression      Limiters, Expanders, and Multi-Band Compressors        Tone and EQ  Placement          Reverb        Effects  DAW Controls          Editing        The Finished MixWhat is Mixing?Now that we’ve learned all of the ins and outs of digital audio, we can get to the fun stuff: mixing and mastering. In this part of the Get Started series, we will talk about the mixing process. Mixing, in it’s simplest definition, is altering the volume of sounds, and adding them all together. Think of a sound engineer at a live show: they are often being yelled at to “turn up the vocals” or “turn down the electric guitar” or “turn down the electric guitar even more please.” Turning different instruments “up and down,” often called changing the levels of each instrument, is the most basic form of mixing. Mixing has become more advanced throughout history, as a result of inventions such as equalization, stereo music, artificial reverberation, and other effects. Due to these advances, modern mixing is also concerned with three other important characteristics of individual sounds within a song: tone, placement in “virtual space,” and effects.Often in a DAW, we will have various tools we can apply to a track, software we call plugins. In this article, we will also go through some of the tools used to achieve different sounds within a song. In addition to plugins, we often have other DAW controls available, including some for sound editing.In this part of Get Started, we will discuss the four fundamental aspects of mixing (levels, tone, placement, and effects) and the tools used to craft them in greater detail. We will also go through some of the basic DAW controls, including editing.LevelsThe most basic of the four aspects of mixing is levels. The level of a track within a mix is simply the volume of that track.Typically, in a DAW, each track has what’s called a fader, which is a slider that dictates the volume of the track. A bunch of faders from Logic X are pictured below:The faders are those grey sliders placed vertically on each track.Levels are important in their own right, as their relation to each other often define the hierarchy of instruments in the mix. Levels also play an important part of simulating a sound coming from a far distance, as we will see later.A typical situation that arises during mixing is that a certain instrument may need to be louder or quieter depending on the part of the song. For example, maybe the kick drum need a little boost during the chorus. Another typical situation is that certain parts of the instrument’s sound may need to be louder or quieter than other parts of the instruments sound. For example, maybe the kick drum is too loud riiiight when the batter head hits the drum head, but is a little too quiet when it is resonating.(as a little side note: I will always be saying  kick drum and not bass drum, since the latter terminology is sometimes confusing when used in the same discussion as bass guitar, which happens quite frequently).The two different types of situations call for two different solutions. In our first situation, we can solve the problem with what is called automation, while in the second situation, we can solve the problem with what is called compression. In the next two sections, we will briefly go over these two tool in our mixing arsenal.AutomationAutomation is used to change the value of a paramter throughout a song. In this case, we can use automation to change the level of a certain track in different sections of a song, or even during certain words, as is common with vocal tracks. Here is an arbitrary example of automation on a kick drum track.As you can see, as the song plays from left to right, the level of the track will change from -5.9 dB, to +1.0 dB, to other values.In most DAWs, you have the ability to draw in different values over the course of a song with a pencil tool (producing shapes known as envelopes). This is what I did to quickly make the automation points in the above picture.Most DAWs also have the functionality to record parameter changes made by the mix engineer as the song plays. When levels in particular are changed during the course of a song by an engineer monitoring the mix, it is typically called riding the fader, for pretty obvious reasongs.Sometimes, though, we have changes we want to make to a sounds’ level that are too fast for us to catch with our hands or a pencil tool, or changes that keep repeating every snare hit, say. It would be very difficult or tedious to use automations to fix these sorts of problems, so for those we turn to compression.CompressionCompression is a tool we can use to “even out” certain parts of a sound. An easy example for this is our kick drum example, where we might want to dull down the attack, where by attack we mean beginning of the sound. Here’s a compressor that has a nice visualization of some common controls:All of the bolded vocab to follow can be found in the second left-most column of this particular compressor (which even has more controls than we will touch upon here).Compressors work by scaling down the level of a track as it gets louder. So if a compressor had what’s often called a ratio control set to “2:1”, it would compress the signal down so it only ever gets half as loud as it normally would. Similarly, a 3:1 ratio would squash the sound a third of it’s normal volume. You get the idea: the higher the ratio, the more compressed the sound. Compressors also often have a threshhold control, which controls the point at which the compressor starts working. If, say, we wanted only the sound spike that happens during the attack of the kick drum to be compressed, and everything underneat that to remain the same, we would set our threshhold between the spike or peak value, but above the resonance of the kick drum after it has been hit.The x-axis (horizontal axis) of our pictured compressor represents the input volume of the compressor during any given moment, and the y-axis represents the output volume. This compressor has a ratio of more than 20:1, which is visualized clearly by the virtually flat output above its theshhold of -40 dB. You can also tell from this graph that the input can go from -100 dB to 0 dB, while the output can never go above -40 dB. These ranges are called the dynamic range of a track. the reason why a compressor is named as it is is because it is compressing the dynamic range.Sometimes we want to bring the compressed track back up in volume, since compression often makes our track sound quieter. We can do this by adding makeup gain. If we give this compressor 40 dB of makeup gain, our graph looks like this:Now, our dynamic range is the same size, it has just moved from -100 dB through -40 dB to -60 dB through 0 dB. This, in effect, makes our track sound louder than it originally did, because it will never fall below -60 dB (as opposed to our input sound, which still has the dynamic range of -100 dB through 0 dB). Some compressors automatically apply makeup gain, so these compressors, counterintuitively, automatically make a sound louder, because they are both compressing and adding makeup gain.Compressors often also have attack and release controls. Imagine the compressor is “squeezing” the sound, as a person might squeeze a stress ball. The attack control dictates how quickly the compressor “squeezes” the sound, and the release control dictates how quickly the compressor “let’s go” of the sound.Compressors are a very tricky thing to hear, especially when they are used subtley, as they ofoten are (it should be noted that a 20:1 ratio as seen is the example above is an exageration of typical values; usually we will have a ratio between 1:2 and 1:5). A huge part of mixing and mastering is grasping exactly how changing these controls changes a sound, and a compressor is one of your two most important tools as a mixing engineer.Limiters, Expanders, and Multi-Band CompressorsCompressors also come in a few different flavors. A standard compressor acts as we have discribed above, but their are other types of compressors with slight differences. A limiter for instance is a compressor with ratio and threshhold controls tied together: as the threshhold decreases, the ratio increases. A popular limiter is shown below:An expander is the opposite of a compressor; it boosts the sound above a certain threshhold, essentially a compressor with a ratio between 0 and 1.There are also multi-band compressors, which are compressors that work differently depending on the frequency of the sound. A de-esser is a type of multi-band compressor tailored to work on frequencies sqpecific to sybilant sounds produced by human speech (think of an “s” sound).Tone and EQIn addition to compression, equalization (EQ), a tool with which you can change the tone of an instrument, is one of the most important tools with mixing and mastering. EQ is used to turn up and down certain frequencies, and often uses several user-defined filters to do so (remember filters?). These filters can be different shapes, which will be outlined in the following sections.The two most basic, and proabably most used filters, are the high pass filter (HPF) and low pass filter (LPF). As their name might suggest, they each pass certain frequencies through them untouched while filtering out other frequencies. A HPF filters out the highs, while a LPF filters out the low. Confusingly, these filters are also sometimes called low cut and high cut filters, respectively. Just take a moment to think about what the terms mean, and this alternate terminology is not too confusing.HPFs and LPFs have a parameter called slope (sometimes called roll-off) which dictates how steep the filters are. They also have a cutoff frequency control which ditates where they start passing highs or lows.Below is a picture of a HPF, with a slope of 24 dB/octave and a cutoff frquency of 79 Hz:And next is a picture of a LPF, with a slope of 24 dB/octave and a cutoff frquency of 7 kHz:In both of these visualizations, we can see our EQ “greying out” (i.e. filtering) frequencies below and above their cutoff frequencies (respectively).Another type of filter is a bell filter, which is named because of it’s bell shape. Bell filters can be turned up or down with a gain control, and also often have a Q control, which dictates how broad the filter is. Bell filters usually have a lower Q value, meaning they are broader in nature. Bell filters are often used gently, and are usually quite subtle.Below is a picture of an EQ using a bell filter with a gain of +9 dB and a Q value of 0.3 (located below the gain value in this picture). It is located at the frequency 1040 Hz:As you can see, with a low Q value, we get a broad, bell shape. In this case, as opposed to the HPF and LPF, the grey region of frequencies are boosted.Alternatively, if a filter has a high Q value, meaning it is very narrow, and is being used to cut (i.e. the gain value is negative), we usually call this a notch figure, aptly named because we are notching out a certain small band of frequencies. Usually a notch filter can be used effectively to cut out a particular frequency in a very transparent way. Below is a notch filter at the same frequency, with a gain of -24 dB and a Q value of 2.1:In this case, again, the grey frequencies are cut.Since a filter with such a high Q value is rarely used to boost frequencies, we don’t really have a name for that case. Call it a “generally bad sounding filter”  if you would like.As should be clear, most changes made to a sound via EQ are subtle or transparent in nature, though they can be  more heavy handed; it all depends on the context and desired result! However, sometimes more radical tonal changes are desired, and for those we often resort to effects, which we will discuss later.PlacementWhen we listen to well produced music in a quiet listening environment, it often sounds like we, the listener, have been placed inside of an actual space, a room where we can hear, for example, a piano to our left, a tambourine to our right and far away, and a singer front and center, sounding very close.Placing sounds within this virtual environment is one of the jobs good mix engineers will spend a lot of time and thought on. Often, this can give an okay mix a serious wow-factor, and (of course) is quite difficult to do effectively. An ability to recreate a realistic virtual environment is often what separates the good mix engineers from the great.The placement of an instrument within the virtual space of a mix is based off of two factors: the panning of the instrument, or its angle from the listener (left, right, center, and anywhere in between), and the depth of the instrument, or how far away the instrument sounds.Usually, a good mix engineer will create a hierarchy of instruments within a mix, having the main parts be front, center, and close, with the supporting parts being panned harder and sound as if they are coming from farther away. Things like lead vocals, kick drum, snare drum, and bass will typically be front, center, and close, while maybe a supporting harmony vocal, hi hat, or guitar might be skewed to one side, and far back. Again though: this all depends on the individual song! These are just some enormously common occurences, not hard and fast rules.ReverbPanning is usually a rather simple to control; most DAWs have a built in panning knob on each track. Depth however, isn’t quite that easy, but there are a few tools at our disposal. When creating a sense of depth, mix engineers often employ, in conjunction with other tricks, a tool known as reverberation, or reverb for short. Reverberation in a room is any sound wave you hear that does not come directly from the source of that sound. Often in addition to the direct sound wave, we will hear sound waves that started at the source, bounced off of a wall, or two, or three, or thirteen, and then arrived at your ear. All this extra sound is what we call reverb (sometimes thought of as echo, but there is a difference. Reverb is often much more difficult to discern compared to reverb. Technically, acoustic echo is a certain type of reverb).Reverb can be added artificially in most DAWs using anything from basic to hugely computational algorithms, and often the quality is dicernable to more experienced ears. Real reverb can also be recorded directly, often with great results; think of the sense of depth you get listening to a well done live recording of a concert.Here is an example of a great reverb engine that comes bundled with Logic X:A reverb plugin, like most effects plugins, has a wet and a dry fader (located on the top right of this picture), used to control the reverb (the wet signal) and the original, direct sound (the dry signal). In addition to this, reverbs have some other common controls including pre-delay, which sets a time delay between the wet a dry signals.We won’t get too much more into how to go about creating a sense of space in your mix here: I have to save some of those details for future series! (which, have I mentioned, you will miss out on if you don’t sign up for the mailing list in the sidebar?).EffectsNow finally, everything else: effects. Almost any change in sound that is not as subtle as the above techniques can be classified as an effect (at least, that is my definition). Any of the tools we talked about earlier (automation, compression, EQ, and reverb) can be used as effects if they are heavy handed or radical enough. More common effects include delay, distortion, chorus, flanging, and pitch shifting, but we won’t go into them much here (again, we’ll go into them in future series if you’re not missing out on those!). They’re the kind of things that most of you have tinkered with, and if you haven’t, they’re loads of fun, and usually come prepackaged with most DAWs. Go turn some knobs an listen to what happens! DAW ControlsIn addition to tools that alter sound, a lot of DAWS have added controls for ease of workflow. A couple of these you may have noticed in the earlier picture of the Logic X faders, notably the buttons labeled M and SThese stand for mute and solo, respectively. Muting a track does exactly what you would expect, and can be used to completely remove the track from the mix. Soloing a track temporarily mutes all other tracks, so that a mix engineer can listen closely to just the track being solo’d. This can be very useful for fine tuning individual sounds within a mix.EditingSometimes a mix engineer might want to align certain tracks, remove squeaks, breaths, or thuds, or pick and choose the best parts from multiple takes. All of these features are luckilly included in most DAWs as editing controls.Usually this can be done in what is often called the workspace of the DAW, pictured below:In the workspace, we can see a layout of particular regions (chunks of recorded audio) within a song. Each horizontal row represents one track, often containing color coded regions of audio for different sections of a song. These regions can be moved around, if, say, two drum tracks aren’t quite lined up because of some recording delay between the two tracks.Regions can also be cut, meaning the region is sliced into two separate regions. This is often done by cutting at the playhead, which is the white vertical line in the above picture which indicates where in the song we are. If we hit play, the song will start playing wherever the playhead lies, and will follow along through the song.Editing is often a manual process, and is helpful for fixing things that we can’t fix with plugins.The Finished MixFinally, we have our levels set, our tone sounding nice, our mix has a sense of space, we’ve added some tasteful effects, and we’ve edited out all of our extraneous noises. Now what?Once a mix is finalized,  it is bounced (exported) to a single audio file. Usually, however, this single file can sound a bit quieter than songs we normally listen to, and may not sound as cohesive as we may like. Luckilly, we can fix these problems through mastering, which we will talk about in the final part of the Get Started Series"
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Digital Audio?",
		link: "/mini-series/getting-started/Get-Started-Part-2/",
		content: "A quick note to the reader: This site is currently under construction, though I am still accepting submissions for a free master and taking clientsContents:  What is Digital Audio?  The Microphone  The Analog to Digital Converter          Analog vs Digital      Audio Quality      The Digital Waveform      Sample Rate (“Digital Time”)      Bit Depth (“Digital Space”)      Clipping        The ComputerWhat is Digital Audio?We live in a crazy time period where we can recreate almost any sound around us, with basically no effort! Most smartphones come preloaded with a voice memo app, giving everyone the power to record sound and play it back at a moment’s notice.Why should you care about the technical details of digital audio? Well… you want your music to sound better than this, don’t you?Learning only a little bit of the technical know-how surrounding audio will put you miles ahead of your fellow musicians, and open your ears to so much more going on underneath the surface of the music you listen to every day.Energy from sound waves go through an obstacle course of sorts as they make their way inside a computer in a process known as recording. In this article, we will follow sound energy through this obstacle course, which we call the signal chain, as it makes its way into a computer. Here is a block diagram with the three basic parts in the recording signal chain:As you can see, we start with the acoustic sound we described in Part 1, and end with digital audio inside your computer. There are three main sections to the recording signal chain: the microphone, the analog to digital converter (or the ADC), and of course the computer.Let’s start with the bridge between the acoustic world and electrical worldThe MicrophoneMicrophones are a type of transducer, which is a device that shifts energy between different forms. In the specific case of the microphone, acoustic energy is shifted into electrical energy, so they are categorized as Electro-acoustic transducers. We won’t go too into detail about how different microphones work, as there a few different methods by which they can convert energy between the electrical and acoustical domains. Essentially, you can think of it this way: a sound wave pushes on something, which in turn pushes on electrons, sending electrons out of the microphone in the exact same pattern that the microphone was pushed by the sound wave. Here is a ubiquoitous microphone, Shure’s SM57:Sound waves come into the metal mesh in front (called a grille), and electrical waves come out of the back of it.Remember how a sound wave is just air molecules vibrating back and forth? When pushed by this vibrating air, the microphone pushes electrons so that they vibrate at the same frequency of the imposing sound wave.So if you could “hear” electrons, and put your electron-hearing-ear on the output of the microphone, you would hear all of that stuff from part one: fundamental frequencies, harmonics, loudness, etc. A piano’s timbre looks the same coming into and going out of the microphone, it is just that on the input air molecules are making this shape, and on the output electrons are making this shape.Different microphones have different characteristics (sometimes called color) due to the distinct mechanisms through which they convert energy. These audible characteristics stem from the fact that microphones do not pick up all frequencies of sound equally; take for example the SM57 above. Included by the manufacturer with this microphone is something called a frequency response, which is exactly what it sounds like: how the microphone responds to certain frequencies (and thus, how it colors the sound). You can see boosts and cuts of certain frequencies in the response below:We can see from the frequency response of the SM57 that it has a relatively flat response in the mid-range, with some boosts in the high end, and a cut in the low end that gently slopes as we get lower (keen observers will note that the scale of this graph is a little weird; we will get there in Part 3, for now, don’t worry about it.) Thus, microphones, just like any part of the signal path, act as a filter. Though filtering in microphones is subtle, it is always there, whether you like it or not.There are also other types of transducers we can use in the place of a microphone for different types of sounds: for example, if you are recording electric guitar, you can use the pickup of the guitar, which is another type of transducer. Or, if you are recording something like a synthesizer, which outputs electronic signals only and no actual sound, you can bypass this whole stage, since you are already starting with electrons! Remember: In this first step, our goal is only to convert sound energy to electrical energy.The Analog to Digital ConverterNext, the signal travels to a soundcard, a mixer, or, what you are probably using to record sound: an audio interface. Each of these has something called an Analog to Digital Converter, or an ADC, which acts as a translator for your computer, which works in 0’s in 1’s, not electrons. The ADC basically says, “Okay, electron here, electron there… that means 110010”.The ADC is a complicated device, but is unrefutably the most important to digital recording. Don’t be intimidated! We will split this part of the signal chain up a little, and explain the ADC more in depth. A lot of concepts explained for the ADC have wide-arcing implications in modern recorded music, so it is imortant to really understand how this thing works.Analog vs DigitalAnalog to digital conversion is hugely important, and not something you want to mess up. To understand a bit more of how this works, we first need to understand what analog and digital mean in the first place.The electrons entering the ADC from the cable exist in the “real world.” This is different from the 1’s and 0’s which exist in the “digital world”. You know this; if you are watching a live video stream of a panda, the panda is not the “real world” panda, but it is a “digital world” copy that looks just about close enough, as far as we can tell. There is no panda inside your computer (really!). We would call the panda inside your computer the “digital panda” and the panda in The Smithsonian’s National Zoo the “analog panda.”When you think analog, think “real world”. Think: there is no smallest chunk.In the “real world” we can keep dividing space forever. If you zoomed in on the real world panda, you could zoom down to its individual hairs, and then further to the proteins making up the hair, and then down to its atoms, and even further; endlessly in fact, if you have a big enough microscope. There is no smallest chunk.However, if you zoomed in on the digital world panda, you would be left with individual pixels, each with the same size, past which you cannot zoom in. In this case, each pixel represents what we call a discrete chunk of space, meaning a finite amount of space.In the “real world,” we can also keep dividing time, forever. A second is pretty small, but a millisecond is even smaller, and a microsecond even smaller than that. If we keep dividing time, there is no point at which we hit a limit. There is no smallest chunk.However, if you slowed down the panda cam, which is digital, you would be left with individual frames, clicking along at a certain speed. In the digital world, these clicks represent a discrete chunk of time (again, a finite amount of time.)So remember: in the digital world, there is always a smallest chunk, and we call these chunks discrete values.Audio QualitySpace and time are two aspects of the “real world” that the digital world absolutely cannot recreate, and this turns out to be crucially important in recreating sound waves, which can get pretty darn fast, and pretty darn small, and on which you could zoom in, forever, both in space and time. In the digital world, we need a smallest, otherwise our computers would not be able to handle the size of the audio file. We need a discrete chunk of electrons to be our smallest chunk. Luckily, there is a point below which our ears can’t hear the difference between real world audio and digital audio if this discrete chunk is small enough. Generally, the smaller you go, the higher the quality of the recording. In audio, we call each discrete chunk a sample. Thus, the more samples you have in any given time or space, the higher the objective quality of your audio.(This is the same language we use for things like drum samples; sample just means a chunk of audio. For this discussion, however, when we use the word sample, we are refering to the smallest chunk. In fact, a drum sample as we normally think of it is actually made up of many, many smaller samples. This is not meant to confuse; only to clarify that the sample we are refering to is the smallest sample)The Digital WaveformThinking about audio as a wave, as we have before, can be very helpful in understanding these two ideas. Plus, audio is usually displayed as a waveform in recording softwares. Take the following digital waveform of a clap I clapped:This is quite intuitive to understand, but worth explaining: At the points where the waveform is very tall, it means that the ADC recieved a big chunk of electrons, and therefore that the microphone sent a big chunk of electrons down the cable to the ADC, and therefore that the microphone recieved a big chunk of air molecules. In other words: something loud just happened! At the points, on the other hand, where the waveform is centered at the 0, there is no sound, as far as the ADC can tell.Intuitively, we understand that the length of these waveforms represents “digital time.” Again, if we zoom in lengthwise, the thinking is the same as zooming in time on our panda cam: we know there is a limit to our time resolution here as well. If we were to zoom in enough, there will be a point at which we could see the wave click along sample by sample, just as we saw the panda cam click by frame by frame.Changes in height of the waveform are what we can think of “digital space.” If we think back to our panda cam, the thinking is the same: we know there is a limit to this resolution. If we were to zoom in enough, there will be a point at which we could see discrete jumps in height of the audio waveform, just as we saw discrete pixels on the panda cam.Let’s dig a little bit deeper into these ideas of “digital time” and “digital space,” and learn some better vocabulary for those terms:Sample Rate (“Digital Time”)The ADC can’t translate electrons to binary at an infinite speed; it has to go back and forth, just as you would imagine a human interpreter going back and forth between worlds in which different languages are spoken. Except this human translator goes really, really, fast, on the order of tens of thousands of times per second.The speed at which the ADC can translate analog waves to digital waves is what we call its sample rate. If you don’t have a fast enough sample rate, you start to miss those rapidly vibrating high frequencies; you start to get less total samples, and the quality of your music suffers.Now, you would think, intuitively, that to record a given frequency, you would at least have to record at the sample rate corresponding to that frequency (to record a 300 Hz tone, for example, you would have to at least record at 300 samples per second.) However, if we put ourselves in the shoes of the ADC, we realize rather quickly why this doesn’t quite work out.Imagine you are assigned a job as an ADC, and you had to figure out the lowest sample rate you could employ to capture a 300 Hz wave.If you tried capturing a 300 Hz wave by sampling at 300 samples per second, you would capture the values represented by the blue line on the graph below.As you can see, the blue line, or what we captured as an ADC, never moves from 0! We have failed to capture our 300 Hz tone; we cannot see 300 Hz with a sampling frequency of 300 Hz. As far as we know, as an ADC, there is no 300 Hz wave.Let’s try again, but this time doubling the sampling frequency. Notice, we get the exact same thing:However, as we will see, any frequency above 600 Hz we will start getting values greater than zero. Here is 601 Hz:You can just start to see nonzero values appearing if you look at the values highlighted by red dots.Let’s try 1200 Hz for a more clear picture:And 2400 Hz:And finally let’s take a ton of samples; here is 44100 Hz:Where our blue digital signal is completely overlayed on our green analog signal. As you can see, any sample rate above what we call the nyquist rate, which is double the rate of the highest frequency wave we are trying to capture, will give us nonzero values, and the higher we go, the better of a picture we get of said wave. Even in the case of the 601 Hz wave, we have enough nonzero points that, if we tried to sketch a sine wave that intercepted all of them, we would have a sine wave at 300 Hz. In fact, computer software is so good at making these sketches, 601 Hz will do just fine to capture a 300 Hz wave. The higher our sample rate, the higher the maximum frequency we can capture. It turns out, 44100 Hz (or 44.1kHz), the sampling frequency we used in the last example, is a typical sampling frequency, one that is probably used by the ADC inside of your audio interface. Why is this used? Remember from part 1: humans can only hear from 20 Hz to 20 kHz. So if we wanted to be able to capture a maximum frequency of 20 kHz, we would need a sample rate above the nyquist rate, which is in this case 40 kHz. 44.1 kHz is well above the 40 kHz, so we can use this sample rate to record anything we can hear without error. The specifics of the number are a bit more “just because.” The music industry could have picked any frequency above 40 kHz, but chose 44.1 kHz as a standard, just because.Bit Depth (“Digital Space”)We know that if we zoomed into the panda cam, we would eventually reach pixels of uniform height and width. So it makes sense that if we zoomed into a waveform, we would find a similar smallest chunk. In audio, we call this smallest chunk a bit. Before we move on, a quick explanation of the binary system is in order:The binary system is just a system of counting that is easy for computers to use. We humans use a system of counting that goes something like “0, 1, 2, 3, 4, 5, 6, 7, 8, 9”. For any value above that, we shift over a decimal place and start again, “10, 11, 12, 13, 14… 21, 21, 23, 24… 31, 32, 33, 34… etc”. You know how it goes from there (I hope). This is called a base ten, or decimal counting system, because we have ten different symbols that we use in representing any given value.In contrast to this system, computers use a base-two counting system (which is just another word for binary), which only uses two symbols: 0 and 1. A computer counts “0, 1” and then shift over a decimal place and start again: “10, 11, 100, 101, 110, 111, 1000, 1001… etc”. These are just different ways of representing the same values: 0 in binary is 0 in decimal, 1 in binary is 1 in decimal, 10 in binary is 2 in decimal, 11 in binary is 3 in decimal, 100 in binary is 4 in decimal, and so on, forever (for more on conversion information, see wikipedia). Each digit in a base-two number, at least for a digital system like a computer, is called a bit, and it turns out that the number of different values we can represent in a binary number is directly related to how many bits (how many digits) it contains: specifically by raising 2 to the number of bits. So if we had a 4-bit number, we would have 2^4 = 8 values we can represent. We can count these ourselves: 000, 001, 010, 011, 100, 101, 110, 111, which, remember, represent 0 through 7 in decimal (note that here we are using leading zeros, which are zeros on the left hand side meant to be placeholders. These are meaningless, just as they are meaningless in base-ten, e.g. when you are writing a date: 04/01/2015 which we understand as 4/1/2015)We need the height of our waveform to be a binary number, since we want to use it inside of a computer. So the smallest possible chunk of height is 1 bit of value 0. The second smallest chunk of height is 1 bit of value 1. Notice we cannot have a value in between 0 and 1; since we are in the digital world, and we are making a bit our smallest chunk, we have to either round up to 1 or down to 0. Say we had a 1-bit ADC. At any given point in time, this ADC looks at it’s input (coming from the microphone) and says, “how many electrons do I have here?”. Unfortunately, our ADC can only count to 1 (since 2^1 = 2 values, which, remember, represent 0 and 1 in binary). So if we know we might see, at a maximum (and this is hypothetical), 60,000 electrons, we can work within the rules of: :   if I see between 0 and 30,000 electrons, that means 0   and if I see between 30,001 and 60,000 electrons, that means 1Which is not all that useful. If we were trying to capture a sine wave, it would look something like:The analog wave we are sampling (green) is a sine wave at 300 Hz, while our sampled wave (blue) doesn’t quite look like a sine wave.Now say we had a 2-bit ADC. At any given point in time, this ADC looks at it’s input (coming from the microphone) and says, “how many electrons do I have here?”. Now, our ADC can count to 3 (since 2^2 = 4 values, which, remember, represent 0, 1, 10, and 11 in binary). So if we know we might see, at a maximum (and this is hypothetical), 60,000 electrons, we can work within the rules of:   if I see 0 electrons, that means 00  if I see between 1 and 20,000 electrons, that means 01  if I see between 20,001 and 40,000 electrons, that means 10  if I see between 40,001 and 60,000 electrons, that means 11Now we are getting somewhere a little closer to our analog wave:(Note that here we are using the base-ten values on the y-axis, not binary)Imagine now we have a 16-bit ADC, or, if we want to learn some vocab, an ADC with a bit-depth of 16 bits. Now, our ADC can count to 2^16 = 65536. Now our resolution is much better; we can represent each individual electron with a bit of it’s own, and even have 5536 bits left to spare. Here is what we get when we sample at that bit-depth:  Finally, our blue sampled wave completely matches our green analog wave (in fact, it completely overlaps it in this picture!)Most ADC’s in use today are either 16-bit or 24-bit, though they often do not get down to resolution as fine as one electron, or anywhere close. It turns out one electron’s worth of sound from a microphone is extremely, if not impossibly difficult to hear in most realistic scenarios. In fact, ADC actually measure electrical power, which is voltage times current, but thinking about things in terms of electrons is much simpler, and will suffice for our discussions.Clippingblah blah clippingThe ComputerFinally, we have a digital audio wave from our ADC that we can save onto our computer! To store these digital sound waves, computers typically use a user-friendly software called a Digital Audio Workstation, or DAW for short. There are many different types of DAWs, ranging from basic freewares such as Audacity to more advanced programs like Logic, Pro Tools, and Cubase. (An important thing to note moving forward is that I primarilly use Logic X, because I work on a Mac, and my experience with Logic has been better for my personal needs than alternative DAWs. But don’t run away if you don’t have any intention of using Logic, or don’t use a Mac! I will jump through flaming hoops to make sure everything I talk about on Get Mastered is applicable accross DAWs. Unfortunately, for Logic users, this means I will refrain from specifics of logic in my general tutorials. If there is demand for it, I can make some more Logic specific tutorials down the line. Perhaps a “Working With Logic” mini-series could happen if ther’s enough clammor for it…)Moving on to Part 3, we will work exclusively inside of the computer. It is important to remember that since we are working inside of a computer from now on, everything is digital. If you skimmed over those big analog vs. digital concepts, it will come back to bite you later when you are dealing with some more advanced features of DAWs; make sure you understand these concepts before moving on! They are tricky, but incredibly important, and your understanding of them will put your music production skills miles ahead of 99% of musicians poking their noses around Garage Band."
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is sound, really though?",
		link: "/mini-series/getting-started/Get-Started-Part-1/",
		content: "A quick note to the reader: This site is currently under construction, though I am still accepting submissions for a free master and taking clientsContents:  What is Sound, Really Though?  What is sound?  Fundamental Frequency, Harmonics, and Pitch          An Example: The synthesizer        Tone  Filters  Loudness  Recreating SoundWhat is Sound, Really Though?Welcome to the Get Started mini-series!In this series, we will review the foundations of the modern music production process. If you’re totally new to this stuff, this is the place for you, and even if you have experience, reviewing the basics never hurts!The series will be split into three sections:Part 1: What is Sound, Really Though? An overview of sound and it’s basic characteristics (hint: you are reading this one)Part 2: What is Digital Audio? How sound waves, electricity, and computers work together (jump to Part 2)Part 3: What is Mixing? An overview of the modern recording process, including tools used in altering recorded music (jump to Part 3)Part 4: What is Mastering? A bit on mix preparation, and your toolkit as a mastering engineer (jump to Part 4)This series will also frequently reference the glossary I am actively cultivating. Don’t just skip over words that confuse you; learning this vocab now will save you headaches later. Plus, you will impress all your friends when you can point out the sidechain compression on the kick in that EDM song at that party (if they can hear you over the bass, that is)So let’s Get Started by answering the question:What is sound?The definition of sound is a little more complicated than it would seem! Sound is our perception of a material’s variation of pressure  versus time through our ears, and the auditory processing parts of our brain. Typically, this material will be air, though - and I hope I don’t sound crazy - we can perceive variations in pressure of other materials too, like water! Just fill up your tub, stick your head underwater, and listen for your roommates to ask you what on earth you are doing. You will be able to hear them, though maybe not as clearly. For now, however, we are just going to deal with air, since most of us aren’t listening to music underwater (most of us).To better understand what this definition means, we need to dip our toes (just our toes!) into physics (only for a second!). Remember: DON’T PANIC. Physics is more afraid of you than you are of it.Pressure is a distribution of force across a given area. In terms of sound, the force comes from air molecules bouncing against the area of your eardrum. The more air molecules you have shoved into one space (i.e. the higher the density of air molecules), the more force they can exert against your eardrum. Conversely, the less air molecules you have shoved into one space (i.e. the lower the density of air molecules), the less force they can exert against your eardrum. It is this variation in air molecule density, that leads to a variation in pressure against our eardrum that we hear as sound.Below is a great animation with which we can visualize the change in density of air molecules that leads to our hearing of sound. Notice how if you pick one position in the sqaure, there is a constantly changing density of air molecules, as the higher pressure “ripples” of air seem to move through that point.In fact, a very useful way of thinking about these “ripples” of higher density air molecules is to call them waves. Specifically, we call this a sound wave, and we can think about it just like a wave created by dropping a rock into a pond.It is a little bit easier to think of these variations in pressure as vibration. Think of when you play music too loudly in your car and the whole thing starts vibrating; that vibration is the direct affect of sound.An easy way to differentiate sound waves from one another is to measure the distance between the points of highest pressure, or the points of highest air molecule density. This is pretty easy to see: All the “darker” circles are high pressure points. Since we are taling about sound waves, we call this distance the wavelength, the length of the wave. Every wavelength, the wave repeats itself. A wavelength is a distance per cycle of the wave.Now something that is going to blow your mind: Keep your eyes focused on one individual air molecule in the animation above (one individual dot). Notice that it is vibrating back and forth at a constant rate (if you are having trouble seeing it, it’s a little easier to see towards the center of the image). Now pick any other dot, and notice that it too is vibrating back and forth at the exact same rate. Pull out a stopwatch if you don’t believe me. Call my bluff, I dare you.This vibration is called a sound’s frequency. Air can vibrate quickly, and air can vibrate slowly.  When air vibrates quickly, it has a high frequency, and when it vibrates slowly, it has a low frequency. The unit of frequency is Hertz (abbreviated Hz), which simply means cycles per second. If you did pull out your stopwatch, and noticed that each dot was returning to the same position once every second, it would have a frequency of 1 Hz (one cycle per second). If a sound is oscillating at 100 Hz, that means that air molecules are moving back and forth 100 times every second.If air vibrates at a frequency anywhere between 20 Hz and 20,000 Hz, we can hear it as sound. Outside of that range, the air is still moving; we just can’t hear it! Other animals can hear different ranges of frequencies that we cannot (dogs, for instance, can hear frequencies above 20,000 Hz)Remember, a wavelength is a distance per cycle, and a frequency is a cycle per second. If a sound wave goes 2 meters per cycle (has a wavelength of 2 meters) and cycles once every one second (has a frequency of 1 Hz), it is moving 2 meters every second. Thus, sound waves also have a speed, or velocity that is a direct relationship between wavelength and frequency. Mathematically, velocity, wavelength, and frequency are related by the equation:Which should seem pretty intuitive if you’ve followed along thus far.Fundamental Frequency, Harmonics, and PitchIn nearly all real life scenarios, sound will not consist of one frequency, but of many, many different frequencies layered upon each other. The lowest of these frequencies is called the fundamental frequency of a sound. Every other frequency within the sound is called a harmonic.Our brains translate these fundamental frequencies into something we call pitch. When you hit a middle C on a piano, you hear a pitch corresponding to that key’s fundamental frequency, which is, in this case, 261.6 Hz. This is the lowest frequency in the sound you hear when you hear a middle C on a piano, but there are still harmonics above this, which are translated by your brain as timbre, or the characteristic of the sound. If you listened to a trumpet playing concert middle C, you are still hearing the fundamental frequency of 261.6 Hz as its pitch, but the trumpet’s harmonics, and thus its timbre, are significantly different from that of a piano.So remember: pitch is your brain’s translation of fundamental frequency, and timbre is your brain’s translation of harmonics 1An Example: The synthesizerPerhaps the easiest instrument on which to visualize fundamental frequencies versus harmonic frequencies is the synthesizer. Most synthesizers start by sending out very basic tones that can then be altered with all of those knobs, buttons, and sliders. These basic tones are usually include easy to recognize shapes, such as sine waves, square waves, triangle waves, and sawtooth waves, represented below (in that order):It’s easy to see where these names come from!Something you might notice is that, though these waves have different shapes, they have the same fundamental frequency; they each repeat 3 times in this picture (if this picture’s length represented 1 second, they would be oscillating at 3 Hz). So, saying the same thing a different way, these all have the same pitch.The difference in the shapes of these waves represents the different timbres you hear listening to these sounds. Just like we talked about with the piano and trumpet, these all have the same fundamental frequencies, but different harmonics.A sine wave has no additional harmonics. If we play a sine wave at 261 Hz (just about middle C), we hear this:SINE WAVE TONEA square wave consists of a sine wave at its fundamental frequency, plus sine waves at odd-order harmonics. In other words, the fundamental (261 Hz), the fundamental times 3 (783 Hz), the fundamental times 5 (1305 Hz), the fundamental times 7 (1827 Hz), all the way through infinity. That sounds like:SQUARE WAVE TONESimilarly, triangle and sawtooth waves consist of similar series of adding together different harmonic patterns of sine waves. In fact, every sound ever heard in the history of forever consists of different combinations of sine waves, added together. In these simple cases, there are patterns we can follow, but for more complex sounds like that of a piano and trumpet, the sequence of harmonics fall out of any pattern and become much more complex, and thus are very difficult to recreate synthetically. That is why if you set a synthesizer to a piano or trumpet setting, you will usually get something that sounds similar to a piano or trumpet, but is easily distinguishable from the original sound.ToneThings start to get tricky, from a vocabulary perspective, when we try to separate timbre and tone. The definitions I am giving are by no means universally agreed upon, but will suffice for the sake of music production.Whereas timbre is the typical collection of harmonics allowing you to distinguish different instruments, tone is a description of the flavor of that timbrel source. Think of different timbres as a specific kinds of food (e.g. french fries vs. pumpkin pie) and tone as certain flavorings of one type of food (salted french fries vs. unsalted french fries).An electric guitar might be bright (lots of high frequency harmonics), distorted (lots of added unnatural harmonics), or warm (lots of pleasant sounding harmonics), for example, but it still has the timbre of an electric guitar. Brightness, distortion, and warmth are some examples of descriptors of tone; there are countless more that are often based more in musical slang than in science (e.g. muddy, clean, thick, crunchy, sparkly)FiltersSomething cool us humans have figured out is that you can filter sounds. A filter is something that changes the strength of certain frequencies in a sound. Just as you might filter your coffee, you are, with a filter, removing parts of a sound, effectively “straining out” certain frequencies. If you think a grand piano has too much high-frequency harmonics, you can filter them out by closing the lid. When you stuck your head underwater earlier, the water was filtering out some of the higher harmonics of your roommates’ voices.What’s more, air itself can act as a filter, if you get enough of it! If you have been to an outdoor concert, or seen a marching band, you’ll notice that the further away you move from the sound source, the more air filters out high frequencies, while you can still hear lower frequency sounds like the bass guitar or bass drum at about the same volume.In all of these cases, a physical object acts as a filter. Though all of these examples acted upon high frequency harmonics, filters can act on any range of frequencies. We tricky humans have figured out how to manipulate audio so that we can eliminate or accentuate certain frequencies in a much more exact way than by filtering sound through physical objects. When done electrically, filtering is called equalization, which we will come back to in Part 3 of Get Started.LoudnessWhen we filter a sound, we are changing the loudness of certain frequencies within that sound. So it makes sense to go back and ask the seemingly innocuous question: what is loudness?Loudness is our brains translation of the strength of a sound, or, in other words, how hard the air molecules which make up a sound are vibrating2. Intuitively, the harder the air is a-shakin’, the louder the sound is a-soundin’, and conversly, the weaker the quieter. A sound’s total loudness relates to the loudness of all of it’s components: not just the fundamental frequency, and not just the harmonics.Recreating SoundOf course,  you wouldn’t be reading this now if recorded music didn’t exist! So how to we make it sound like there is a rock band living inside of our computer speakers, ready to play any song at our will?The trick lies in converting those wiggling air molecules into wiggling electrons. Just like music in the air consists of different frequencies of air molecule vibrations, music in your computer consists of different frequencies of electron vibrations.How does your computer do this? Continue on to Part 2: Digital Audio to find out            note I keep saying brain’s translation here; your brain is not always exact. This is outside the scope of this series, but I will come back to it some day. Just like most things concerning psychoacoustics, it’s pretty trippy, and just a little too confusing for now. &#8617;              see above &#8617;      "
	}); 
    
	JEKYLL_POSTS.push({
		title: "What is Get Mastered?",
		link: "/about/What-Is-Get-Mastered/",
		content: "A quick note to the reader: This site is currently under construction, though I am still accepting submissions for a free master and taking clientsIt’s a great time to be a self-producing musician. But despite easy access to recording programs, so many resources remain inaccessible to musicians looking to self-produce, due to a combination of limited capital investment, recording and mixing myths, and old industry separations of power.Get Mastered is a blog devoted to teaching artists how easy it can be to produce high quality, professional sounding recordings by beefing up that pink matter between your ears with recording and mixing knowledge.What’s with the name?This blog is named Get Mastered because in addition to being a resource to learn how to record and mix music, it is also the absolute best place to get your music mastered.Find out how I can give you such low rates here.What if I am totally, totally new to this stuff?This blog is aimed at musicians of any experience level. The material is explained in a way that is simple to understand, yet still covers the full breadth of the topic, including dips into the worlds of music theory, writing, acoustics, psychoacoustics, math (dear god no!), and signal processing, among other bordering fields.  To get the most out of the blog, however, it is super helpful to have some basic foundational knowledge of digital audio recording and production. If words like:  Mastering  Equalization, or  Ditheringsound foreign to you, you should check out the Get Started mini-series. This is where I explain a lot of that pesky jargon, in a way that is very easy to digest.The last section of this mini-series also includes the software you will need to do your own recording and mixing, so if you don’t already have your own setup, make sure you don’t miss out on that!What if I want to tell you that you’re wrong, or dumb or something?I have comment sections at the bottom of all my posts. Get Mastered is meant to be a collaborative learning environment; it doesn’t work unless you make your thoughts known, boost good comments, and, of course… ask questions!(I take care to answer all questions in the comments as quickly as possible!)Why do you do this anyway? What’s in it for you, audio-boy?I do all of this because it’s what I love to do. I maintain the website, I master the music, I write the posts; it’s all a one-man production, and it’s a lot of hard work. If you like what I’m doing, show your appreciation by being a part of the community, telling your friends, and…  asking questions!"
	}); 
    
</script>
<script src="/js/search.js"></script>
<script>
	new jekyllSearch({
		selector: "#results",
		properties: ["title", "content"]
	});
</script>
    </div>
    <div class="row-fluid hidden-lg hidden-xl">
      <!DOCTYPE html>

<div id="mc_embed_signup">
<div class="row-fluid" style="margin: 0px 10px 30px 0px; padding: 0px;">
<form id="contact-form" method="post" action="//getmastered.us10.list-manage.com/subscribe/post?u=bb14eb93467fa9f89bc914f09&amp;id=1d85fcd133" style="padding: 0px 0px;" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <h4 style="color: #777; text-align: right; margin: 0px; padding: 5px;">Want 10% off your first job?</h4>
  <!--   <p style="font-size: 14px; color: #777; text-align: right; margin: 0px; padding: 5px;">Sign up for the mailing list and get exclusive discounts:</p> -->
    
    <div id="mc_embed_signup_scroll" style="padding: 10px 0px 10px 30px;">
                <div class="container-fluid">
                        <div class="row">
                            <div class="form-group">
                                <textarea class="form-control" name="EMAIL" id="mce-EMAIL" rows="1" cols="100" required="required" placeholder="Enter your email to get exclusive discounts"></textarea>
                            </div>
                            <div id="mce-responses" class="clear">
                    <div class="response" id="mce-error-response" style="display:none"></div>
                    <div class="response" id="mce-success-response" style="display:none"></div>
                </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
                            
                            <button type="submit" name="subscribe" class="btn btn-skin pull-right bg-dark" value="Subscribe" id="mc-embedded-subscribe">Submit</button>
                     </div>
            </div>
        </div>
</form>
</div>
</div>

    </div>
	</div>
</div>



    <div class="row equal bg-gray" style="padding: 0px 0px 20px 0px; margin: 0px;">
        <div class="col-lg-3 sidebar hidden-md hidden-sm hidden-xs" style="padding: 0px 0px 0px 0px; margin: 0px 0px 0px 0px;">
        </div>
        <div class="col-lg-9 bg-gray" style="padding-top: 40px; padding-left: 20px; padding-right: 20px;">
            <p style="color: #999;"><b>Get Mastered</b> is a free site, with no ads. And it's a whole lot of work. 
                <br>
                If you think a friend could benefit from the post you're reading, please help me out and <b>share it</b>, either in person or on social media</p>
        </div>

    </div>

    <!-- Core JavaScript Files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/jquery.easing.min.js"></script>    
    <script src="/js/jquery.scrollTo.js"></script>
    <script src="/js/wow.min.js"></script>
    <!-- Custom Theme JavaScript -->
    <script src="/js/custom.js"></script>

  </body>

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MSTFPS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-MSTFPS');</script>
<!-- End Google Tag Manager -->

<!-- Facebook bull -->
<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

</html>




